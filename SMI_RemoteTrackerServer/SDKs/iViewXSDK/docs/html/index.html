<!-- HTML header for doxygen 1.8.2-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.2"/>
<title>iView X SDK: iView X™ API Documentation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="extra.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><a href="http://www.smivision.com/"><img alt="Logo" src="smi_logo.png"/></a></td>
  <td style="padding-left: 0.5em;">
   <div id="projectname">iView X SDK
   &#160;<span id="projectnumber">4.4</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.2 -->
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li class="current"><a href="index.html"><span>iView&#160;X&#160;SDK</span></a></li>
      <li><a href="modules.html"><span>Reference</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('index.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">iView X™ API Documentation </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="intro"></a>
Introduction</h1>
<h2><a class="anchor" id="welcome"></a>
Welcome to the iView X™ SDK 4.4 Guide!</h2>
<h3><a class="anchor" id="aboutIVXSDK"></a>
About iView X™ SDK</h3>
<p>The iView X™ Software Development Kit ("SDK") provides an Application Interface ("API") for communication between your software application and your SMI eye tracking device, allowing you to create applications that take advantage of the powerful features offered by SensoMotoric Instruments ("SMI") eye tracking devices. Specifically, the SDK was designed for SMI customers who wish to add eye tracking into their own applications. Using the interface provided within the SDK you can control SMI eye tracking devices and retrieve eye tracking data online.</p>
<p><a class="anchor" id="apiLayer"></a></p>
<p>The figure below shows hard- and software components of the eye tracking system. Your application connects via the API with the iView eye tracking server.</p>
<div class="image">
<img src="APILayers.png" alt="APILayers.png"/>
<div class="caption">
API Layers</div></div>
<p><b>Your Application:</b> Your software using the API to interact with the eye tracking device. You can develop your own application or integrate 3rd party applications into your eye tracking system.</p>
<p><b>iView X™ API:</b> Programmable interface to provide access to eye tracking device. iView X™ API is part of the iView X™ SDK. A common C Interface is provided, but you can use any programming language to build your own eye tracking application. Please check <a class="el" href="index.html#supLang">Supported Programming and Scripting Languages</a> for details.</p>
<p><b>Eye tracking Server:</b> iView eye tracking server application which collects the data from the eye tracking device and provides the data via the iView X™ API. </p>
<dl class="section note"><dt>Note</dt><dd>Depending on your system, the iView eye tracking server functionality is provided by different binaries. For Hi-Speed, RED, etc., this is iView X. For RED-m and RED-OEM, this is the eyetracking_server. For RED250mobile, REDn Scientific and REDn Professional this is iViewNG-Server. To improve readability the term <b>iView eye tracking server</b> is used as a generic name for this software component.</dd></dl>
<p><b>Eye tracking Device:</b> eye tracking device by SMI. Please check <a class="el" href="index.html#supDev">Supported Eye Tracking Devices</a> for a list of supported devices.</p>
<p><br/>
 <hr/>
<h3><a class="anchor" id="aboutGuide"></a>
About the Guide</h3>
<p>The SDK Guide provides a practical introduction to developing applications using the SDK and documentation about major SDK features. It includes instructions for setting up your SDK environment and a function reference, which outlines each available function. Additionally, the manual gives a brief overview on the included examples for each major platform. If you want to start working with your eye tracking device immediately, you may proceed to the section <a class="el" href="index.html#gettingStarted">Getting Started</a>.</p>
<p><br/>
 <hr/>
<h3><a class="anchor" id="whatsNew"></a>
What's New?</h3>
<p>If you are familiar with previous versions of the iView X™ SDK, please notice the highlights of this version:</p>
<ul>
<li>Tracking Modes: Access to binocular and monocular eye tracking features have been improved. The section <a class="el" href="index.html#trackingModes">Binocular and Monocular Tracking Modes</a> describes the new tracking modes.</li>
<li>Smart Calibration: Data quality gets assessed for each calibration point and each eye to improve gaze accuracy. Details can be found in <a class="el" href="index.html#secSmartCalib">Smart Calibration</a>.</li>
<li>Return values of certain fuctions have been added for a more detailed error analysis and their documentation has been improved.</li>
</ul>
<p>In addition to this guide, the SDK includes release notes which can be found in the installation directory:<br/>
 <b>\iView X SDK\docs</b><br/>
 In the release notes you can find a complete list of the improvements and bug fixes, helping you get the most from each release.</p>
<dl class="section note"><dt>Note</dt><dd>New API features and fixes introduced since the iView X™ SDK version 4.2 only affect the devices <b>RED250mobile</b>, <b>REDn Scientific</b> and <b>REDn Professional</b>. For all other devices the functionality of the iView X™ SDK version 3.6 is retained.</dd></dl>
<p><br/>
 <hr/>
<h2><a class="anchor" id="sysreq"></a>
System Requirements</h2>
<h3><a class="anchor" id="supDev"></a>
Supported Eye Tracking Devices</h3>
<p>The following SMI Eye Tracking Devices are supported in this release:</p>
<table class="doxtable">
<tr>
<th>Supported Eye Tracking Device </th><th>Frame Rate [Hz]</th></tr>
<tr>
<td>HED </td><td>50 Hz / 200 Hz </td></tr>
<tr>
<td>HED HT </td><td>50 Hz / 200 Hz </td></tr>
<tr>
<td>Hi-Speed </td><td>240 Hz (monocular) </td></tr>
<tr>
<td>Hi-Speed </td><td>350 Hz (monocular / binocular) </td></tr>
<tr>
<td>Hi-Speed </td><td>500 Hz (monocular / binocular) </td></tr>
<tr>
<td>Hi-Speed </td><td>1250 Hz (monocular) </td></tr>
<tr>
<td>Hi-Speed Primate </td><td>500 Hz / 1250 Hz (monocular / binocular) </td></tr>
<tr>
<td>MRI LR </td><td>50 Hz </td></tr>
<tr>
<td>MEG </td><td>50 Hz / 250 Hz </td></tr>
<tr>
<td>RED 4 (Firewire) </td><td>50 Hz / 60 Hz </td></tr>
<tr>
<td>RED (USB) </td><td>60 Hz / 120 Hz </td></tr>
<tr>
<td>RED250 </td><td>60 Hz / 120 Hz / 250 Hz </td></tr>
<tr>
<td>RED500 </td><td>60 Hz / 120 Hz / 250 Hz / 500 Hz </td></tr>
<tr>
<td>RED-m </td><td>60 Hz / 120 Hz </td></tr>
<tr>
<td>RED-OEM </td><td>30 Hz - 60 Hz </td></tr>
<tr>
<td>RED250mobile </td><td>60 Hz / 120 Hz / 250 Hz </td></tr>
<tr>
<td>REDn Scientific </td><td>30 Hz / 60 Hz </td></tr>
<tr>
<td>REDn Professional </td><td>30 Hz / 60 Hz </td></tr>
</table>
<dl class="section note"><dt>Note</dt><dd>Please note that SMI Eye Tracking Glasses (ETG) are not supported with this version of iView X™ SDK. Please visit <a href="http://www.smivision.com/software">http://www.smivision.com/software</a> for more information.</dd></dl>
<p><br/>
 <hr/>
<h3><a class="anchor" id="supLang"></a>
Supported Programming and Scripting Languages</h3>
<p>The iView X™ SDK can be used with most of the programming and scripting languages that are capable of importing dynamic link libraries (DLLs). These include, but are not limited to:</p>
<ul>
<li><a class="el" href="index.html#ccpp">C/C++</a></li>
<li><a class="el" href="index.html#csharpTut">C#</a></li>
<li><a class="el" href="index.html#matlab">Matlab</a></li>
<li><a class="el" href="index.html#eprimeSetup">E-Prime</a></li>
<li><a class="el" href="index.html#pythonSetup">Python</a></li>
<li><a class="el" href="index.html#nbsSetup">NBS Presentation</a></li>
<li><a class="el" href="index.html#unity">Unity</a></li>
</ul>
<p><br/>
 <hr/>
<h3><a class="anchor" id="suppOS"></a>
Supported Operating Systems</h3>
<p>This SDK installer contains Windows 32-bit and 64-bit binaries. The SDK application files are installed into:</p>
<table class="doxtable">
<tr>
<th>Type of Windows Version </th><th>Folder of iView X™ SDK</th></tr>
<tr>
<td>32-bit </td><td>C:\Program Files\SMI\iView X SDK </td></tr>
<tr>
<td>64-bit </td><td>C:\Program Files (x86)\SMI\iView X SDK </td></tr>
</table>
<p>The iView X™ SDK version 4.4 is designed to run on the following operating systems:</p>
<table class="doxtable">
<tr>
<th>Operating System </th><th>Notes</th></tr>
<tr>
<td>Windows 7 32-bit/64-bit </td><td>Supported </td></tr>
<tr>
<td>Windows 8, Windows 8.1 32-bit/64-bit </td><td>Supported </td></tr>
<tr>
<td>Windows 10, 32-bit/64-bit </td><td>Supported </td></tr>
</table>
<p><br/>
 <hr/>
<h1><a class="anchor" id="gettingStarted"></a>
Getting Started</h1>
<h2><a class="anchor" id="setup"></a>
How to Set up the SDK Environment</h2>
<p>In the following sections you will learn how to set up your SDK environment, about the various function available in the SDK, and how to create your first basic eye tracking application based on the provided examples.</p>
<p><br/>
 <hr/>
<h3><a class="anchor" id="downloading"></a>
Downloading</h3>
<p>You can download the latest recommended release of the SDK from the SMI Software Downloads page: <a href="http://www.smivision.com/software">http://www.smivision.com/software</a>.</p>
<p><br/>
 <hr/>
<h3><a class="anchor" id="runInst"></a>
Running the Installer</h3>
<dl class="section note"><dt>Note</dt><dd>The SDK has to be installed on the same computer as your eye tracking application. In a Single PC setup (see <a class="el" href="index.html#singledualpc">Single PC and Dual PC Setup</a>), this will be the same computer that is physically connected to your eye tracker and runs the iView™ software.</dd></dl>
<p>After you have downloaded the SDK installer package, run</p>
<p><b>SMI iView X SDK.exe</b></p>
<p>to begin the installation. When the files have been unpacked, the SDK License Agreement will appear — it contains important information about the terms under which we supply the SDK. Agree to it if you would like to proceed with the installation. If you had a previous installation it will first be removed before the new version of the SDK is installed on your computer. Please wait for the installation to complete. The installation process may take a few minutes.</p>
<dl class="section note"><dt>Note</dt><dd>The SDK is already included in some RED-OEM Developer Editions, in which case there is no need to install iView X™ SDK.</dd></dl>
<p><br/>
 <hr/>
<h2><a class="anchor" id="rundemo"></a>
Running the Demo</h2>
<p>Once you have completed installation of the SDK, you are ready to start developing applications. To learn more about the potential of the iView X™ SDK you may start with a demo application that shows some of the features provided by the API .</p>
<p>To start the demo application, please</p>
<ol type="1">
<li>Connect your eye tracking device and start the eye tracking software. Depending on your device type, this is usually the iView eye tracking server.</li>
<li>Run the <b>HelloEyetracker.exe</b> application. It can be found here:</li>
</ol>
<table class="doxtable">
<tr>
<th>Operating System </th><th>Location</th></tr>
<tr>
<td>32-bit </td><td>C:\Program Files\SMI\iView X SDK\Examples\VS C#\HelloEyetracker\Application </td></tr>
<tr>
<td>64-bit </td><td>C:\Program Files (x86)\SMI\iView X SDK\Examples\VS C#\HelloEyetracker\Application </td></tr>
</table>
<p>Press <b>Connect</b> to establish the connection between the HelloEyetracker application and the iView eye tracking server.</p>
<div class="image">
<img src="HelloEyetracker.png" alt="HelloEyetracker.png"/>
</div>
 <p>Once a connection has been established, tracking monitor and gaze data will be streamed automatically. You can visualize your gaze position by enabling <b>Live Gaze</b> checkbox. Gaze accuracy can be improved by performing a calibration, details about the calibration procedure can be found in the corresponding chapter <a class="el" href="index.html#calibration">Calibration</a>.</p>
<p>The source code for this application is available as a part of the SDK. Please have a look into the section <a class="el" href="index.html#csharpTut">Tutorial: Displaying the gaze cursor with C#</a> to learn more about using C# and Microsoft Visual Studio to access the iView X™ SDK.</p>
<p><br/>
 <hr/>
<h2><a class="anchor" id="secCommonWorkflow"></a>
Common Workflow</h2>
<p>This section describes the common workflow of eye tracking applications using the iView X™ API. In the subsequent sections you learn how to realize this workflow in your individual environment or programming language. We recommend to become familiar with the common workflow first and to study the details of your environment afterwards.</p>
<p>A common eye tracking application performs the following steps:</p>
<div class="image">
<img src="Workflow_API.png" alt="Workflow_API.png"/>
</div>
 <p>For the detailed description we use a C-like programming language syntax to explain the calls to API functions. To learn how to call API functions from your preferred programming language please refer to the corresponding section.</p>
<p><b>1. Connect to the iView eye tracking server</b></p>
<p>To establish a connection call <a class="el" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0">iV_Connect</a>. The parameters shown here connect to iView eye tracking server running on the same PC as your application. They should work with most systems and configurations. For details of the network setup, please see <a class="el" href="index.html#singledualpc">Single PC and Dual PC Setup</a> and your eye tracking device's manual.</p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0" title="Establishes a connection to the iView eye tracking server.">iV_Connect</a>( <span class="stringliteral">&quot;127.0.0.1&quot;</span>, 4444, <span class="stringliteral">&quot;127.0.0.1&quot;</span>, 5555);</div>
</div><!-- fragment --><p>After a connection has been established, the application can be used to control the iView eye tracking server's behavior or to retrieve online data for further processing.</p>
<p><b>2. Run a calibration</b></p>
<p>The next step in the common workflow is a calibration. A calibration is used to determine participant-specific physiological characteristics to initialize gaze mapping and to optimize eye tracking performance. Usually, a sequence of points is presented where the participant has to gaze at. Details about the calibration can be found in the section <a class="el" href="index.html#calibration">Calibration</a>.</p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga94789a33a7956ed855be25f4920aec9b" title="Starts a calibration procedure.">iV_Calibrate</a>();</div>
</div><!-- fragment --><p>After the calibration has been performed the system is ready to calculate and provide gaze data.</p>
<p><b>3. Present a stimulus and gather eye tracking data</b></p>
<p>There are two ways to handle eye tracking data:</p>
<p><b>Online Data Analysis:</b> Your application retrieves and processes eye tracking data online. This can be used for interaction paradigms, e.g. gaze based control of user interfaces. The code snippet shows a loop where gaze data is polled and streamed to a console.</p>
<div class="fragment"><div class="line"><span class="keywordflow">while</span> (getchar() != <span class="charliteral">&#39;q&#39;</span>)</div>
<div class="line">{</div>
<div class="line">    <a class="code" href="struct_sample_struct.html" title="This struct provides information about an eye data sample.">SampleStruct</a> sampleData;</div>
<div class="line">    <a class="code" href="group__functions.html#ga2fd180b432ff6ab1d8064cfd4448247e" title="Updates data in rawDataSample with current eye tracking data.">iV_GetSample</a>( &amp;sampleData);</div>
<div class="line">    cout &lt;&lt; <span class="stringliteral">&quot;Left Eye&#39;s Gaze Data X: &quot;</span> &lt;&lt; sampleData.<a class="code" href="struct_sample_struct.html#a47129a38f95739a52f6a4bc58c1a4fb5" title="stores information of the left eye (see EyeDataStruct for more information)">leftEye</a>.<a class="code" href="struct_eye_data_struct.html#a29197bfdd2bf54be3087cb8cad2bead8" title="horizontal gaze position on screen [pixel]">gazeX</a> &lt;&lt; <span class="stringliteral">&quot; Y: &quot;</span> &lt;&lt; sampleData.<a class="code" href="struct_sample_struct.html#a47129a38f95739a52f6a4bc58c1a4fb5" title="stores information of the left eye (see EyeDataStruct for more information)">leftEye</a>.<a class="code" href="struct_eye_data_struct.html#a5ffbb74e0547dcf930b982323b21b708" title="vertical gaze position on screen [pixel]">gazeY</a> &lt;&lt; endl;</div>
<div class="line">}</div>
</div><!-- fragment --><p>Gaze coordinates stored in <b>sampleData</b> can be used to realize gaze based interaction instead. For details about polling and other ways to retrieve online data please refer to <a class="el" href="index.html#pollingvscallbacks">Polling vs. Callbacks</a>.</p>
<p><b>Offline Data Analysis:</b> Your application triggers iView eye tracking server to record eye tracking data into an IDF file, which can be analyzed afterwards. This approach is used if data from a larger set of participants shall be analyzed or compared, or if no gaze based interaction is needed. SMI provides powerful tools for offline data analysis; please check your BeGaze manual for further information.</p>
<p>To start data recording, call</p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga9fb78edcbd972e37e31c99cee6cc867a" title="Starts gaze data recording and scene video recording (if connected eye tracking device is HED)...">iV_StartRecording</a>();</div>
</div><!-- fragment --><p>When done with recording, call</p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga7c5046a1475133d1ab15d3d299878d65" title="Stops gaze data recording and scene video recording (if connected eye tracking device is HED)...">iV_StopRecording</a>();</div>
</div><!-- fragment --><p>and finally</p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga9a80d3724f377a7ee903736282fcad1b" title="Writes recorded data buffer to disc.">iV_SaveData</a>( <span class="stringliteral">&quot;eyedata.idf&quot;</span>, <span class="stringliteral">&quot;shortDescription&quot;</span>, <span class="stringliteral">&quot;username&quot;</span>, 0);</div>
</div><!-- fragment --><p>to save the recoded data to a local file. For more details, please refer to the <a class="el" href="index.html#creatingidfs">Creating IDF files</a> section.</p>
<p><b>4. Close the connection</b></p>
<p>To shutdown the connection, call</p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga63ea57fa0861aa4255d64822a2103087" title="Disconnects from iView eye tracking server.">iV_Disconnect</a>();</div>
</div><!-- fragment --><p>before closing your application.</p>
<p><br/>
 <hr/>
<h1><a class="anchor" id="secdevapps"></a>
Developing Advanced Applications</h1>
<h2><a class="anchor" id="singledualpc"></a>
Single PC and Dual PC Setup</h2>
<p>iView X™ API handles control flow and data flow between your application and iView eye tracking server. Control commands are submitted from your application and are addressed to the iView eye tracking server. Data is produced by the iView eye tracking server and is sent to your application. Therefore, a bidirectional connection is needed. Therefore, your application and iView eye tracking server have to configure the communication channels. Please refer to your system's manual to learn how to set up network connection at iView eye tracking server side.</p>
<p>For your applications, there are two ways to communicate with the iView eye tracking server via the iView X™ API:</p>
<ul>
<li>Single PC Setup</li>
<li>Dual PC Setup</li>
</ul>
<p>Both methods are described below.</p>
<h3>Single PC Setup</h3>
<p>Your application and eye tracking device are running on the same PC.</p>
<div class="image">
<img src="1PCSetup.png" alt="1PCSetup.png"/>
</div>
 <p>There are two ways to establish a connection between your application and the iView eye tracking server running on the same PC.</p>
<ul>
<li>Using <a class="el" href="group__functions.html#ga94b848674c8a3167296a220e920b58d3">iV_ConnectLocal</a> does not require any parameters for the application or additional settings on server side.</li>
<li>With <a class="el" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0">iV_Connect</a> you can establish a connection using settings for <b>localhost</b>. Please read the following instructions for details:</li>
</ul>
<p>Although no hardware network connection is used, your application has to setup a <b>localhost</b> network connection to access iView eye tracking server. Typically, this is realized using the IP address <b>127.0.0.1</b>. The port settings have to be mirrored:</p>
<ul>
<li><b>SendPort</b> from your application has to be the <b>ReceivePort</b> from iView eye tracking server. Default port number is <b>4444</b>.</li>
<li><b>ReceivePort</b> from your application has to be the <b>SendPort</b> from iView eye tracking server. Default port number is <b>5555</b>.</li>
</ul>
<p>To establish a connection between your application and the eye tracking server, parameters of <a class="el" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0">iV_Connect</a> are:</p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0" title="Establishes a connection to the iView eye tracking server.">iV_Connect</a>( sendIPAddress, sendPort, recvIPAddress, receivePort);</div>
</div><!-- fragment --><p>In the described case <a class="el" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0">iV_Connect</a> has to be called from your application in the following way:</p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0" title="Establishes a connection to the iView eye tracking server.">iV_Connect</a>( <span class="stringliteral">&quot;127.0.0.1&quot;</span>, 4444, <span class="stringliteral">&quot;127.0.0.1&quot;</span>, 5555);</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>For systems running with SMI iViewRED 4.2 or higher, it is no longer required to define the parameters <b>recvIPAddress</b> and <b>receivePort</b>. The connection can be established using</dd></dl>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0" title="Establishes a connection to the iView eye tracking server.">iV_Connect</a>( <span class="stringliteral">&quot;127.0.0.1&quot;</span>, 4444, NULL, 0);</div>
</div><!-- fragment --><p>The connection has to be terminated using:</p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga63ea57fa0861aa4255d64822a2103087" title="Disconnects from iView eye tracking server.">iV_Disconnect</a>();</div>
</div><!-- fragment --><p><a class="el" href="group__functions.html#ga94b848674c8a3167296a220e920b58d3">iV_ConnectLocal</a> establishes a connection similar to <a class="el" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0">iV_Connect</a>. With <a class="el" href="group__functions.html#ga94b848674c8a3167296a220e920b58d3">iV_ConnectLocal</a> port settings are handled automatically. There is no need to run <a class="el" href="group__functions.html#ga63ea57fa0861aa4255d64822a2103087">iV_Disconnect</a> to close a connection created with <a class="el" href="group__functions.html#ga94b848674c8a3167296a220e920b58d3">iV_ConnectLocal</a>.</p>
<h3>Dual PC Setup</h3>
<p>Your application and iView eye tracking server are running on different PCs. Both PCs are connected via Ethernet. Low level communication between your application and iView eye tracking server via iView X™ API is realized via UDP/IP network communication.</p>
<div class="image">
<img src="2PCSetup.png" alt="2PCSetup.png"/>
</div>
 <p>For this example we assume the following IP addresses: </p>
<table class="doxtable">
<tr>
<th align="left">PC </th><th align="center">IP address</th></tr>
<tr>
<td align="left">Stimulus PC </td><td align="center">192.168.1.1 </td></tr>
<tr>
<td align="left">Eye Tracking PC </td><td align="center">192.168.1.2 </td></tr>
</table>
<p>In iView eye tracking server, the network settings have to be configured as follows:</p>
<table class="doxtable">
<tr>
<th align="left">Direction </th><th align="center">IP address </th><th align="center">Port</th></tr>
<tr>
<td align="left">Receive/Listen </td><td align="center">192.168.1.2 </td><td align="center">4444 </td></tr>
<tr>
<td align="left">Send To </td><td align="center">192.168.1.1 </td><td align="center">5555 </td></tr>
</table>
<p><a class="el" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0">iV_Connect</a> has to be called from your application in the following way:</p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0" title="Establishes a connection to the iView eye tracking server.">iV_Connect</a>( <span class="stringliteral">&quot;192.168.1.2&quot;</span>, 4444, <span class="stringliteral">&quot;192.168.1.1&quot;</span>, 5555);</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>For systems running with SMI iViewRED 4.2 or higher, it is no longer required to define the parameters <b>recvIPAddress</b> and <b>receivePort</b>. The connection can be established using</dd></dl>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0" title="Establishes a connection to the iView eye tracking server.">iV_Connect</a>( <span class="stringliteral">&quot;192.168.1.2&quot;</span>, 4444, NULL, 0);</div>
</div><!-- fragment --><p>The connection has to be terminated using:</p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga63ea57fa0861aa4255d64822a2103087" title="Disconnects from iView eye tracking server.">iV_Disconnect</a>();</div>
</div><!-- fragment --><p><br/>
 <hr/>
<h2><a class="anchor" id="multiclient"></a>
Connecting with Multiple Applications</h2>
<dl class="section note"><dt>Note</dt><dd>This feature is only available for RED-m and RED-OEM devices. It requires iView X™ SDK version 3.4.6 or newer and iView eye tracking server version 2.11.65 or newer.</dd></dl>
<p>To run multiple applications or multiple instances of the same application in parallel, each running instance has to establish its own communication channel.</p>
<p>The mechanism described in <a class="el" href="index.html#singledualpc">Single PC and Dual PC Setup</a> allows configuration of one or at the maximum two communication channels - depending on the underlying eye tracking software's capabilities.</p>
<div class="image">
<img src="multiclient.png" alt="multiclient.png"/>
</div>
 <p><br/>
 <hr/>
<h2><a class="anchor" id="redgeometry"></a>
Setting up RED Geometry</h2>
<p>The SDK can be used to configure the position of the RED relative to the stimulus screen. Using the proper settings for the geometry is required to reach the optimal gaze accuracy.</p>
<p>There are two ways to position the RED relative to the screen</p>
<ul>
<li><b>Monitor Attached</b> mode describes the usage of the mounting brackets to attach the device close to the screen. This mode is available for certain devices only.</li>
<li>In <b>Standalone</b> mode the RED position is independent from the stimulus screen.</li>
</ul>
<table class="doxtable">
<tr>
<th>Device </th><th>Available Modes</th></tr>
<tr>
<td>RED60, RED120, RED250, RED500 </td><td><b>Monitor Attached</b>, <b>Standalone</b> </td></tr>
<tr>
<td>RED-m, RED-OEM, RED250mobile, REDn Professional, REDn Scientific </td><td><b>Standalone</b> </td></tr>
</table>
<h3>Monitor Attached Mode - RED60, RED120, RED250 and RED500</h3>
<p>For monitor attached mode, the following parameters from the structure <a class="el" href="struct_r_e_d_geometry_struct.html">REDGeometryStruct</a> are relevant:</p>
<table class="doxtable">
<tr>
<th>Parameter </th><th>Value</th></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#a65b6c8c1af6521a8af546c6bc92f2dac">REDGeometryStruct::redGeometry</a> </td><td><a class="el" href="group__enums.html#gga14e5c25b00a7c2cab5a31b36ed5847b0a44ab952d466a79a1b7c3f954abde218a">REDGeometryEnum::monitorIntegrated</a> </td></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#aa51ed571854fc3095b46ebb056ceab6c">REDGeometryStruct::monitorSize</a> </td><td><code>19</code> or <code>22</code> </td></tr>
</table>
<p>The function <a class="el" href="group__functions.html#ga30cc4d362dba3e31fb70773f570e5722">iV_SetREDGeometry</a> configures the settings related to the display device. The monitor attached mode is not available for RED-m.</p>
<h3>Stand Alone Mode - RED60, RED120, RED250 and RED500</h3>
<p>The data structure <a class="el" href="struct_r_e_d_geometry_struct.html">REDGeometryStruct</a> contains all required geometrical parameters. The function <a class="el" href="group__functions.html#ga30cc4d362dba3e31fb70773f570e5722">iV_SetREDGeometry</a> configures the stand alone geometry.</p>
<div class="image">
<img src="REDOperationMode.png" alt="REDOperationMode.png"/>
</div>
 <p>The following steps are necessary to setup the RED in stand-alone mode:</p>
<ol type="1">
<li>Remove the RED from the monitor and mount it on the stand-alone foot.</li>
<li>Position your external screen (beamer, TV, monitor) as follows:<ul>
<li>The screen has to be planar</li>
<li>The screen has to be at right angle with the floor</li>
<li>The screen bottom line has to be parallel to the floor</li>
<li>RED is in the horizontal middle of the display device</li>
</ul>
</li>
<li>Enter a profile name and the following geometrical dimensions of your setup into <a class="el" href="struct_r_e_d_geometry_struct.html">REDGeometryStruct</a></li>
<li>Call the function <a class="el" href="group__functions.html#ga30cc4d362dba3e31fb70773f570e5722">iV_SetREDGeometry</a> including the <a class="el" href="struct_r_e_d_geometry_struct.html">REDGeometryStruct</a> as parameter to iView eye tracking server</li>
</ol>
<table class="doxtable">
<tr>
<th>Parameter </th><th>Value</th></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#a65b6c8c1af6521a8af546c6bc92f2dac">REDGeometryStruct::redGeometry</a> </td><td><a class="el" href="group__enums.html#gga14e5c25b00a7c2cab5a31b36ed5847b0af544d1983223d0d5a8811c09e5542b0e">REDGeometryEnum::standalone</a> </td></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#a0e6f4ac6e58eb0cef590eed1b1bc8a14">REDGeometryStruct::setupName</a> </td><td>Profile name </td></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#af97d960d058612cd29327a5f497a9575">REDGeometryStruct::stimX</a> </td><td>Screen width [mm] </td></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#a97a3b4b386cc9be6dff85d38b68637bb">REDGeometryStruct::stimY</a> </td><td>Screen height [mm] </td></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#a6a730640d2359cd42b82bba80f4c6b58">REDGeometryStruct::stimHeightOverFloor</a> </td><td>Distance floor to screen [mm] </td></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#a642ff535cff6bcd6913517c498ba41e9">REDGeometryStruct::redHeightOverFloor</a> </td><td>Distance floor to RED [mm] </td></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#a85291f299f0d26970a0089944d945bc6">REDGeometryStruct::redStimDist</a> </td><td>Distance RED to screen [mm] </td></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#a48e2f3df9a3ef5be0039f08cf586f7bf">REDGeometryStruct::redInclAngle</a> </td><td>RED inclination angle [degree] </td></tr>
</table>
<h3>Stand Alone Mode - RED-m, RED-OEM, RED250mobile, REDn Professional and REDn Scientific</h3>
<p>Note: Although attached to a screen, the geometrical set up has to be regarded as "stand alone" due to advanced options for configuration.</p>
<div class="image">
<img src="OEMPlaneConfigurationSDKManual.png" alt="OEMPlaneConfigurationSDKManual.png"/>
</div>
 <p>The following steps are necessary to setup the RED in stand alone mode:</p>
<ol type="1">
<li>Position your RED and your screen (beamer, TV, monitor) as follows:<ul>
<li>RED is in the horizontal middle of the display device</li>
<li>Position and align the RED in a way that the user's head is in the middle of the tracking box.</li>
</ul>
</li>
<li>Enter a profile name and the following geometrical dimensions of your setup into <a class="el" href="struct_r_e_d_geometry_struct.html">REDGeometryStruct</a></li>
<li>Call the function <a class="el" href="group__functions.html#ga30cc4d362dba3e31fb70773f570e5722">iV_SetREDGeometry</a> including the <a class="el" href="struct_r_e_d_geometry_struct.html">REDGeometryStruct</a> as parameter to iView eye tracking server</li>
</ol>
<table class="doxtable">
<tr>
<th>Parameter </th><th>Value</th></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#a65b6c8c1af6521a8af546c6bc92f2dac">REDGeometryStruct::redGeometry</a> </td><td><a class="el" href="group__enums.html#gga14e5c25b00a7c2cab5a31b36ed5847b0af544d1983223d0d5a8811c09e5542b0e">REDGeometryEnum::standalone</a> </td></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#a0e6f4ac6e58eb0cef590eed1b1bc8a14">REDGeometryStruct::setupName</a> </td><td>Profile name </td></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#af97d960d058612cd29327a5f497a9575">REDGeometryStruct::stimX</a> </td><td>Screen width [mm] </td></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#a97a3b4b386cc9be6dff85d38b68637bb">REDGeometryStruct::stimY</a> </td><td>Screen height [mm] </td></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#a500cf773ca2ddbd0d17e7487ddeb9f78">REDGeometryStruct::redStimDistHeight</a> </td><td>Vertical distance RED to stimulus screen [mm] </td></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#a3c25e42d0d0747e6a6010eaac00b88d6">REDGeometryStruct::redStimDistDepth</a> </td><td>Horizontal distance RED to stimulus screen [mm] </td></tr>
<tr>
<td><a class="el" href="struct_r_e_d_geometry_struct.html#a48e2f3df9a3ef5be0039f08cf586f7bf">REDGeometryStruct::redInclAngle</a> </td><td>RED inclination angle [degree] </td></tr>
</table>
<p><br/>
 <hr/>
<h2><a class="anchor" id="trackingModes"></a>
Binocular and Monocular Tracking Modes</h2>
<p>The iView X™ SDK is able to handle and setup different tracking modes which are supported by SMI RED devices. Depending on your device type, some of the modes described here may be unavailable.</p>
<p><br/>
 <hr/>
<h3><a class="anchor" id="smartBinoc"></a>
Smart Binocular</h3>
<p>The default tracking mode is <b>Smart Binocular</b> and is aimed to track and calculate the gaze of both eyes of the participant, but will tolerate if just one eye is visible to the eye tracker. In this case the system is still able to track the participant, to calculate the gaze cursor, and compensate the head movements. To enable it during run time, the following function needs to be called:</p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga0579a278d029de7d7aa4bf2c3095ab81" title="Sets iView eye tracking server tracking parameters.">iV_SetTrackingParameter</a>( ET_PARAM_EYE_BOTH, ET_PARAM_SMARTBINOCULAR, 0);</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>Since <b>SMI iViewRED 4.2</b>, the assignment of the left or right eye channel within the smart binocular is deprecated. <code>ET_PARAM_EYE_LEFT</code> and <code>ET_PARAM_EYE_RIGHT</code> can no longer be used with <code>ET_PARAM_SMARTBINOCULAR</code>. A similar behavior can be accessed with using the <b>monocular</b> mode, see <a class="el" href="index.html#monoc">Monocular</a> for details.</dd></dl>
<p><br/>
 <hr/>
<h3><a class="anchor" id="smartTracking"></a>
Smart Tracking</h3>
<p><b>Smart Tracking</b> mode is designed to optimally track participants regardless of whether both eyes are equally strong or one eye is stronger than the other. The better eye is selected if it is much better than the other. If both eyes perform equally, binocular data is available. The function <a class="el" href="group__functions.html#ga773c4aa7b4f775d91a00e75115a9c162">iV_GetAccuracyImage</a> can be used to visualize the performance of both eyes. <a class="el" href="group__functions.html#gae1dd00a53e92f1dcee847d9ffb8902b2">iV_GetGazeChannelQuality</a> can be used to retrieve numerical values that allow assessing the data quality. To set up this tracking mode, call:</p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga0579a278d029de7d7aa4bf2c3095ab81" title="Sets iView eye tracking server tracking parameters.">iV_SetTrackingParameter</a>( ET_PARAM_EYE_BOTH, ET_PARAM_SMARTTRACKING, 0); </div>
</div><!-- fragment --><p>The eye tracker requires a validation or a calibration with at least five calibration points to assess the accuracy of the left and the right eye data channels.</p>
<p><br/>
 <hr/>
<h3><a class="anchor" id="binoc"></a>
Binocular</h3>
<p>The <b>Binocular</b> tracking mode requires both eyes to be tracked. It will not tolerate one visible eye only. To enable it during run time, call:</p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga0579a278d029de7d7aa4bf2c3095ab81" title="Sets iView eye tracking server tracking parameters.">iV_SetTrackingParameter</a>( ET_PARAM_EYE_BOTH, ET_PARAM_BINOCULAR, 0);</div>
</div><!-- fragment --><p><br/>
 <hr/>
<h3><a class="anchor" id="monoc"></a>
Monocular</h3>
<p>The <b>Monocular</b> mode is designed to track participants with just one active eye. Gaze is calculated for this eye only, gaze data of the other eye is ignored. Data for the active eye is written to the IDF file or sent to client via the API, data channel for the other eye contains zeroed data. To set up this tracking mode, call:</p>
<p>To set up this tracking mode, call: </p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga0579a278d029de7d7aa4bf2c3095ab81" title="Sets iView eye tracking server tracking parameters.">iV_SetTrackingParameter</a>( ET_PARAM_EYE_RIGHT, ET_PARAM_MONOCULAR, 0); </div>
</div><!-- fragment --><p>for the right eye or</p>
<div class="fragment"><div class="line"><a class="code" href="group__functions.html#ga0579a278d029de7d7aa4bf2c3095ab81" title="Sets iView eye tracking server tracking parameters.">iV_SetTrackingParameter</a>( ET_PARAM_EYE_LEFT, ET_PARAM_MONOCULAR, 0); </div>
</div><!-- fragment --><p>for the left eye.</p>
<p><br/>
 <hr/>
<h2><a class="anchor" id="calibration"></a>
Calibration</h2>
<p>A calibration is used to determine participant-specific physiological characteristics to initialize gaze mapping and to optimize eye tracking performance. Usually, a sequence of points is presented on which the participant has to gaze at. The iView X API manages the calibration process and provides a build-in visualization displayed on the Stimulus PC. However, the build-in visualization can be switched of and replaced by a custom made if required (see subsection <a class="el" href="index.html#nonvisCalib">Custom Visualization</a>).</p>
<p>The usual workflow includes some setup before running the calibration:</p>
<div class="fragment"><div class="line"><a class="code" href="struct_calibration_struct.html" title="Use this struct to customize the calibration and validation behavior.">CalibrationStruct</a> calibrationParameters;</div>
<div class="line">calibrationParameters.<a class="code" href="struct_calibration_struct.html#a91adbae58580a1e10617cae7995d3dc3" title="select calibration method (default: 5) a bit mask is used to specify a new calibration workflow (see ...">method</a> = 9; <span class="comment">// use 9 calibration points</span></div>
<div class="line"><span class="comment">//... further parameter customization</span></div>
<div class="line"><a class="code" href="group__functions.html#ga859a42317115fef52e4904f515cd8f9e" title="Sets the calibration and validation visualization parameter.">iV_SetupCalibration</a>(&amp; calibrationParameters);</div>
<div class="line"><a class="code" href="group__functions.html#ga94789a33a7956ed855be25f4920aec9b" title="Starts a calibration procedure.">iV_Calibrate</a>();</div>
</div><!-- fragment --><h3><a class="anchor" id="secAcceptCalibPoints"></a>
Accepting Calibration Points</h3>
<p>During calibration (or validation), it needs to be ensured that a participant is fixating upon a calibration point at the moment it is beeing accepted by the iView eye tracking server. The recommended and easiest way is to set the acceptance option to <b>automatically</b>. This mode assumes that the participant is gazing at the calibration points while they are presented. For that, the willingness of cooperation by the participant is required.</p>
<p>Instead of letting the server accept calibration points automatically, the participant can <b>manually</b> tell the server, by when he is fixating a calibration point. In this mode, the iView eye tracking server waits for an unlimited time for the acceptance signal from user. After receiving an acceptance signal, the server will try to track the participant gazing at the calibration point. If the participant cannot be tracked and Smart Calibration is activated, the iView eye tracking server will drop the current calibration point (see subsection secSmartCalib). Otherwise it will expect the user to accept the calibration point again, until the requirements for fixations are fulfilled.</p>
<p>In <b>Semi automatically</b> mode the iView eye tracking server uses manual acceptance for the first calibration point only. Subsequent calibration points are accepted automatically.</p>
<p>If - for any reason - one calibration point cannot be accepted, data acquisition for that certain point can be aborted with <a class="el" href="group__functions.html#ga1507d249e3f3202d09d308c1557522ca">iV_AbortCalibrationPoint</a>. The calibration procedure will continue with the subsequent point.</p>
<h3><a class="anchor" id="secSmartCalib"></a>
Smart Calibration</h3>
<p>With <b>Smart Calibration</b> enabled when accepting a calibration point, the calibration process waits for required fixations for two seconds. If any fixation is found unreliable (e.g. when the user was not really fixating that point), the fixation data will be dropped and the calibration point will not be used to calculate gaze correction parameters. This helps by removing bad fixations containing big error.</p>
<dl class="section note"><dt>Note</dt><dd>When using Smart Calibration in combination with automatic acceptance of calibration points, there is a timeout of two seconds for each point. If the system is unable to track the eyes in that time, data from the current calibration point is discarded.</dd></dl>
<p>After a calibration in Smart Calibration mode, the client application can retrieve information about the actual usage of calibration points for each eye using <a class="el" href="group__functions.html#ga91f2253e17ccd87c200e6a5afeee3f88">iV_GetAccuracy</a>, <a class="el" href="group__functions.html#ga773c4aa7b4f775d91a00e75115a9c162">iV_GetAccuracyImage</a> or <a class="el" href="group__functions.html#ga191c57a89738c0a18f53799ee15f390c">iV_ShowAccuracyMonitor</a>.</p>
<h3><a class="anchor" id="nonvisCalib"></a>
Custom Visualization</h3>
<p>The iViewX API allows to create a custom visualization for the calibration or validation. In this case calibration points are not drawn by the iViewX API. Instead the application has to take care of a proper visualization. In order to enable a custom visualization, <a class="el" href="group__functions.html#ga859a42317115fef52e4904f515cd8f9e">iV_SetupCalibration</a> has to be called with the <code>visualization</code> member of the <a class="el" href="struct_calibration_struct.html">CalibrationStruct</a> set to <code>0</code>:</p>
<div class="fragment"><div class="line"><a class="code" href="struct_calibration_struct.html" title="Use this struct to customize the calibration and validation behavior.">CalibrationStruct</a> calibrationParameters;</div>
<div class="line">calibrationParameters.<a class="code" href="struct_calibration_struct.html#aa495c3da3cb652c30892ffd773aa0cde" title="draw calibration/validation by API (default: 1)">visualization</a> = 0; </div>
<div class="line"><span class="comment">//... further parameter customization</span></div>
<div class="line"><a class="code" href="group__functions.html#ga859a42317115fef52e4904f515cd8f9e" title="Sets the calibration and validation visualization parameter.">iV_SetupCalibration</a>(&amp; calibrationParameters);</div>
<div class="line"></div>
<div class="line"><span class="comment">//... calibration with custom visualization</span></div>
</div><!-- fragment --><p>The <code>visualization</code> parameter also changes the behavior of the functions <a class="el" href="group__functions.html#ga94789a33a7956ed855be25f4920aec9b">iV_Calibrate</a> and <a class="el" href="group__functions.html#ga58b0607192e6d42a60be6061ae1ff681">iV_Validate</a>. If the parameter is set to <code>1</code> (default), both functions are called synchronously (blocking). This means, the functions do not return until the calibration or validation process has finished. However, if <code>visualization</code> is set to <code>0</code> to disable the visualization, both functions are called asynchronously (non-blocking). The following code will be executed while the calibration or validation process is ongoing.</p>
<p>There are two methods of implementing a custom visualization: <b>polling</b> the current calibration point with <a class="el" href="group__functions.html#ga7b0c085be14605b1bcc520f36ecd53ef">iV_GetCurrentCalibrationPoint</a> or using a <b>callback function</b> passed to <a class="el" href="group__functions.html#gaee41bc8259b09335ae7d9a3fca44fabe">iV_SetCalibrationCallback</a>.</p>
<p>The following example illustrates how to realize a visualization by polling:</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> x, y;</div>
<div class="line"><span class="keywordtype">int</span> calibrationPointAccepted = 0;</div>
<div class="line"><span class="keywordtype">int</span> calibrationPointAborted = 0;</div>
<div class="line"><a class="code" href="struct_calibration_point_struct.html" title="This struct provides information about the position of calibration points.">CalibrationPointStruct</a> currentCalibrationPoint = { 0 };</div>
<div class="line"><span class="keywordtype">int</span> lastCalibrationPointNumber = 0;</div>
<div class="line"></div>
<div class="line"><a class="code" href="group__functions.html#ga94789a33a7956ed855be25f4920aec9b" title="Starts a calibration procedure.">iV_Calibrate</a>(); <span class="comment">// or iV_Validate()</span></div>
<div class="line"></div>
<div class="line"><span class="keywordflow">do</span> {</div>
<div class="line"></div>
<div class="line">    <a class="code" href="group__functions.html#ga7b0c085be14605b1bcc520f36ecd53ef" title="Updates data in currentCalibrationPoint with the current calibration point position.">iV_GetCurrentCalibrationPoint</a>(&amp;currentCalibrationPoint);</div>
<div class="line"></div>
<div class="line">    <span class="comment">// check if new calibration point</span></div>
<div class="line">    <span class="keywordflow">if</span> (currentCalibrationPoint.<a class="code" href="struct_calibration_point_struct.html#a7106e2abc437ad981830d14176d15f09" title="number of calibration point the first calibration point has the number 1, the last one has the number...">number</a> &gt; 0 &amp;&amp;</div>
<div class="line">        lastCalibrationPointNumber != currentCalibrationPoint.<a class="code" href="struct_calibration_point_struct.html#a7106e2abc437ad981830d14176d15f09" title="number of calibration point the first calibration point has the number 1, the last one has the number...">number</a>) {</div>
<div class="line"></div>
<div class="line">        lastCalibrationPointNumber = currentCalibrationPoint.<a class="code" href="struct_calibration_point_struct.html#a7106e2abc437ad981830d14176d15f09" title="number of calibration point the first calibration point has the number 1, the last one has the number...">number</a>;</div>
<div class="line"></div>
<div class="line">        x = currentCalibrationPoint.<a class="code" href="struct_calibration_point_struct.html#a7759b29e7af15459632645dc7cbb059b" title="horizontal position of calibration point [pixel]">positionX</a>;</div>
<div class="line">        y = currentCalibrationPoint.<a class="code" href="struct_calibration_point_struct.html#a1a668795d324254281b152fdc17d7cac" title="vertical position of calibration point [pixel]">positionY</a>;</div>
<div class="line">        <span class="comment">// To implement: draw calibration screen with calibration point with </span></div>
<div class="line">        <span class="comment">// coordinates x, y</span></div>
<div class="line">    }</div>
<div class="line"></div>
<div class="line">    <span class="comment">// To implement: </span></div>
<div class="line">    <span class="comment">// Check if calibration point was accepted by the user, e.g. by key press. </span></div>
<div class="line">    <span class="comment">// In that case, set calibrationPointAccepted = 1. This is only </span></div>
<div class="line">    <span class="comment">// necessary if the calibration acceptance mode is manual or </span></div>
<div class="line">    <span class="comment">// semi-automatic.</span></div>
<div class="line">    <span class="comment">// Check if calibration was aborted by the user, e.g. by key press.</span></div>
<div class="line">    <span class="comment">// In that case, set calibrationPointAborted = 1</span></div>
<div class="line">    </div>
<div class="line">    <span class="keywordflow">if</span> (calibrationPointAborted) {  </div>
<div class="line">        <a class="code" href="group__functions.html#ga3c112bbfb954dede4899d3f71f229e54" title="Aborts a calibration or validation if one is in progress.">iV_AbortCalibration</a>();</div>
<div class="line">    }</div>
<div class="line">        </div>
<div class="line">    <span class="keywordflow">if</span> (calibrationPointAccepted) { </div>
<div class="line">        <a class="code" href="group__functions.html#ga94ea9623a4f24a315278d1badc706032" title="Accepts a calibration or validation point if the calibration or validation is in progress.">iV_AcceptCalibrationPoint</a>();</div>
<div class="line">        </div>
<div class="line">        <span class="comment">// reset for next calibration point</span></div>
<div class="line">        calibrationPointAccepted = 0;   </div>
<div class="line">    }</div>
<div class="line">    </div>
<div class="line"><span class="comment">// The calibration or validation is still active if and only if </span></div>
<div class="line"><span class="comment">// calibration points are positive.</span></div>
<div class="line">} <span class="keywordflow">while</span> (currentCalibrationPoint.<a class="code" href="struct_calibration_point_struct.html#a7106e2abc437ad981830d14176d15f09" title="number of calibration point the first calibration point has the number 1, the last one has the number...">number</a> &gt; 0)</div>
</div><!-- fragment --><p>The following example illustrates how to realize a visualization by callbacks:</p>
<div class="fragment"><div class="line"><span class="comment">// define callback function</span></div>
<div class="line"><span class="keywordtype">int</span> __stdcall myCalibrationCallback (<span class="keyword">struct</span> <a class="code" href="struct_calibration_point_struct.html" title="This struct provides information about the position of calibration points.">CalibrationPointStruct</a> calibrationPoint) {</div>
<div class="line"></div>
<div class="line">    <span class="keywordtype">int</span> x;</div>
<div class="line">    <span class="keywordtype">int</span> y;</div>
<div class="line"></div>
<div class="line">    <span class="keywordflow">if</span> (calibrationPoint.<a class="code" href="struct_calibration_point_struct.html#a7106e2abc437ad981830d14176d15f09" title="number of calibration point the first calibration point has the number 1, the last one has the number...">number</a> &gt; 0) {</div>
<div class="line">    </div>
<div class="line">        x = calibrationPoint.<a class="code" href="struct_calibration_point_struct.html#a7759b29e7af15459632645dc7cbb059b" title="horizontal position of calibration point [pixel]">positionX</a>;</div>
<div class="line">        y = calibrationPoint.<a class="code" href="struct_calibration_point_struct.html#a1a668795d324254281b152fdc17d7cac" title="vertical position of calibration point [pixel]">positionY</a>; </div>
<div class="line">    </div>
<div class="line">        <span class="comment">// To implement: draw calibration screen with calibration point at x, y</span></div>
<div class="line">    }</div>
<div class="line">    </div>
<div class="line">    <span class="keywordflow">return</span> 1;</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line"><span class="comment">// ...</span></div>
<div class="line"></div>
<div class="line"><span class="comment">// main program</span></div>
<div class="line"></div>
<div class="line"><a class="code" href="group__functions.html#gaee41bc8259b09335ae7d9a3fca44fabe" title="Sets a callback function for the calibration and validation process.">iV_SetCalibrationCallback</a>(&amp;myCalibrationCallback);</div>
<div class="line"><a class="code" href="group__functions.html#ga94789a33a7956ed855be25f4920aec9b" title="Starts a calibration procedure.">iV_Calibrate</a>();</div>
<div class="line"></div>
<div class="line"><span class="comment">// To implement: </span></div>
<div class="line"><span class="comment">// Wait if calibration point was accepted by the user, e.g. by keypress.</span></div>
<div class="line"><span class="comment">// In that case, call iV_AcceptCalibrationPoint(). This is only necessary if</span></div>
<div class="line"><span class="comment">// the calibration acceptance mode is manual or semi-automatic.</span></div>
<div class="line"><span class="comment">// Wait if calibration was aborted by the user, e.g. by keypress.</span></div>
<div class="line"><span class="comment">// In that case, call iV_AbortCalibration().</span></div>
<div class="line"><span class="comment">// ...</span></div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ul>
<li>The acceptance of calibration points is only required for manual calibration point acceptance mode or for the first callibration point using semi-automatic acceptance mode (see <a class="el" href="index.html#secAcceptCalibPoints">Accepting Calibration Points</a>).</li>
<li>When a new calibration point is active, depending on the device, it takes a few hundred milliseconds until the calibration point can be accepted in order to prevent premature acceptance.</li>
</ul>
</dd></dl>
<h3><a class="anchor" id="recalibration"></a>
Recalibration</h3>
<p><a class="el" href="group__functions.html#gabaf455035ef53222bec1ae6ee808f85f">iV_RecalibrateOnePoint</a> can be used to recalibrate a certain point, in case a participant did not fixate that point properly.</p>
<p><br/>
 <hr/>
<h2><a class="anchor" id="validation"></a>
Validation</h2>
<p>To evaluate the calibration quality, the participant may perform a validation after the calibration. For that, <a class="el" href="group__functions.html#ga58b0607192e6d42a60be6061ae1ff681">iV_Validate</a> has to be called. A sequence of four points is presented to the user, similar to the calibration procedure. The validation calculates the difference between the presented validation points and the measured gaze points. Overall results of the validation can be retrieved with <a class="el" href="group__functions.html#ga91f2253e17ccd87c200e6a5afeee3f88">iV_GetAccuracy</a>, <a class="el" href="group__functions.html#ga773c4aa7b4f775d91a00e75115a9c162">iV_GetAccuracyImage</a> or <a class="el" href="group__functions.html#ga191c57a89738c0a18f53799ee15f390c">iV_ShowAccuracyMonitor</a>.</p>
<p>Similar to the calibration procedure <a class="el" href="group__functions.html#ga1507d249e3f3202d09d308c1557522ca">iV_AbortCalibrationPoint</a> can be used to abort the data acquisition for a validation points.</p>
<p><br/>
 <hr/>
<h2><a class="anchor" id="aoi"></a>
Areas of Interest (AOI)</h2>
<p>The Area of Interest (AOI) feature allows you to define rectangular objects within the stimulus for high level gaze and fixation analysis. Your application is informed whenever the gaze enters or leaves an AOI, or a fixation event was detected within an AOI.</p>
<div class="image">
<img src="AOI.png" alt="AOI.png"/>
</div>
 <p>There are multiple ways to retrieve information about gaze hits or fixation hits on AOIs:</p>
<ul>
<li><b>IDF File</b> If IDF recording is running a message will be send to the IDF data stream (see offline data analysis in <a class="el" href="index.html#secCommonWorkflow">Common Workflow</a>).</li>
<li><b>LPT Port</b> AOI interaction can be signaled to the LPT port. To define the port in use, call the function <a class="el" href="group__functions.html#ga29ba17bd08e41ac87bbb783323fed3c4">iV_DefineAOIPort</a>. <b>outputValue</b> from <a class="el" href="struct_a_o_i_struct.html">AOIStruct</a> can be used to define the TTL value that is send if the corresponding AOI is hit by gaze or fixation. This is useful if you wish to trigger and synchronize other measurement devices with the gaze position.</li>
<li><b>Callbacks</b> A Callback function can be defined that is called whenever a gaze event happens in an AOI.</li>
</ul>
<p>For more details, see reference information for <a class="el" href="group__functions.html#ga9f8c4ba9e4c9c43e75f048ea889ae281">iV_DefineAOI</a> and <a class="el" href="struct_a_o_i_struct.html">AOIStruct</a> how to define AOIs. Note, that some devices do not support AOIs (see <a class="el" href="group__func_device.html">Function and Device Overview</a>).</p>
<p><br/>
 <hr/>
<h2><a class="anchor" id="pollingvscallbacks"></a>
Polling vs. Callbacks</h2>
<p>iView X™ API provides two ways to access eye tracking data online:</p>
<ul>
<li>Polling</li>
<li>Callbacks</li>
</ul>
<p>The following table shows the interface functions to be used when realizing certain tasks with polling or callbacks.</p>
<table class="doxtable">
<tr>
<th align="left">Task </th><th align="left">Polling </th><th align="left">Callbacks</th></tr>
<tr>
<td align="left">Get event data </td><td align="left"><a class="el" href="group__functions.html#ga1645cdab024a85f13a1959ba375897d2">iV_GetEvent</a> </td><td align="left"><a class="el" href="group__functions.html#ga15746542212ed406cf38da851bcb14f6">iV_SetEventCallback</a> </td></tr>
<tr>
<td align="left">Get sample data </td><td align="left"><a class="el" href="group__functions.html#ga2fd180b432ff6ab1d8064cfd4448247e">iV_GetSample</a> </td><td align="left"><a class="el" href="group__functions.html#gae6d482222983f5f9e6f8735a0d130906">iV_SetSampleCallback</a> </td></tr>
<tr>
<td align="left">Get current calibration point </td><td align="left"><a class="el" href="group__functions.html#ga7b0c085be14605b1bcc520f36ecd53ef">iV_GetCurrentCalibrationPoint</a> </td><td align="left"><a class="el" href="group__functions.html#gaee41bc8259b09335ae7d9a3fca44fabe">iV_SetCalibrationCallback</a> </td></tr>
<tr>
<td align="left">Get eye images </td><td align="left"><a class="el" href="group__functions.html#ga41d8982b73ea6d3ccf56501ed5fb5ac0">iV_GetEyeImage</a> </td><td align="left"><a class="el" href="group__functions.html#ga38158d074cd288bb5b09cabf189ef0d6">iV_SetEyeImageCallback</a> </td></tr>
<tr>
<td align="left">Get HED scene images </td><td align="left"><a class="el" href="group__functions.html#gaa2b86de79b0dcb2abf57d2e5bf728cda">iV_GetSceneVideo</a> </td><td align="left"><a class="el" href="group__functions.html#gaee1e64ec36f40d5180faf162ec4bb549">iV_SetSceneVideoCallback</a> </td></tr>
<tr>
<td align="left">Get RED Tracking Monitor Image </td><td align="left"><a class="el" href="group__functions.html#ga51ac9bc8602d2e19ce68fc9834e1d06b">iV_GetTrackingMonitor</a> </td><td align="left"><a class="el" href="group__functions.html#ga50c02f04942c16df39cbe6bd3faa08b6">iV_SetTrackingMonitorCallback</a> </td></tr>
<tr>
<td align="left">Get AOI Hits </td><td align="left"><a class="el" href="group__functions.html#gae0dc7bbe737e10152d112b647a88c36b">iV_GetAOIOutputValue</a> </td><td align="left"><a class="el" href="group__functions.html#ga9746231e4f567466a87b0a4f4cac34d3">iV_SetAOIHitCallback</a> </td></tr>
</table>
<p>Both methods provide different features, advantages and disadvantages. With <b>polling</b> your application has full control about the calling frequency of the polling function. Returned data will always contain the latest known values, independently if they have</p>
<ul>
<li>not been updated</li>
<li>updated once</li>
<li>updated several times</li>
</ul>
<p>since the last call.</p>
<p><b>Callback</b> Functions are called by the API as often as the data is updated by the underlying iView eye tracking server. Restrictions may apply due to system load.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Callback functions are not called as long as the previously executed callback of the same type has not finished. Therefore, it is recommended to put only very short and fast executing commands into callbacks.</li>
<li>Callbacks are not available in all programming languages.</li>
<li>Callback functions are called from different threads. Therefore, the code within callback functions has to be thread safe.</li>
<li>While Polling for images (Eye Image, Tracking Monitor, Scene Video, Accuracy Image) its recommended to use only one <a class="el" href="struct_image_struct.html" title="Use this struct to get raw eye image, raw scene video image, raw tracking monitor image or accuracy i...">ImageStruct</a> instance for each data set.</li>
</ul>
</dd></dl>
<p><br/>
 <hr/>
<h2><a class="anchor" id="creatingidfs"></a>
Creating IDF files</h2>
<p>A readily available possibility to record data for later analysis is to create IDF files. IDF files store gaze samples and messages and can be imported from the data analysis Software BeGaze that is part of SMI's Experiment Suite 360°. BeGaze provides powerful tools for data analysis. With BeGaze it is also possible to prepare and export recorded data for various other third party analysis tools. Note: for this section, it is helpful to be familiar with BeGaze. Please consult the BeGaze manual for further details.</p>
<h3><a class="anchor" id="recordingdata"></a>
Recording Data</h3>
<p>IDF data recording can be started with <a class="el" href="group__functions.html#ga9fb78edcbd972e37e31c99cee6cc867a">iV_StartRecording</a> and stopped with <a class="el" href="group__functions.html#ga7c5046a1475133d1ab15d3d299878d65">iV_StopRecording</a>. It can be paused with <a class="el" href="group__functions.html#gad5644021b75f2011e3d205b48346fe69">iV_PauseRecording</a> and continued with <a class="el" href="group__functions.html#gac9dbab07ddc1d7550a38daadf5e40c58">iV_ContinueRecording</a>. Thus, the eye tracking server can be in three different recording states: <b>recording off</b>, <b>recording on</b> and <b>recording paused</b>. The following diagram illustrates how different recording related function calls change the recording state of the eye tracking server.</p>
<div class="image">
<img src="recording_states.jpg" alt="recording_states.jpg"/>
</div>
 <p><a class="el" href="group__functions.html#ga9a80d3724f377a7ee903736282fcad1b">iV_SaveData</a> saves recorded data into an IDF file. The IDF files are always saved on the machine running the iView eye tracking server. Internally, the eye tracking server holds a data buffer collecting data when recording is on. The buffer is cleared when <a class="el" href="group__functions.html#ga9a80d3724f377a7ee903736282fcad1b">iV_SaveData</a> is called. It is possible to clear the buffer without creating an IDF file by calling <a class="el" href="group__functions.html#ga651c63470895aa6d098851a5725058fe">iV_ClearRecordingBuffer</a>. It is good practice to call <a class="el" href="group__functions.html#ga651c63470895aa6d098851a5725058fe">iV_ClearRecordingBuffer</a> before using <a class="el" href="group__functions.html#ga9fb78edcbd972e37e31c99cee6cc867a">iV_StartRecording</a> at the beginning of an experiment because the buffer might not be empty due to a previously aborted experiment. The following code illustrates the common work-flow:</p>
<div class="fragment"><div class="line"><span class="comment">// At the beginning of an experiment:</span></div>
<div class="line"></div>
<div class="line"><span class="comment">// ensure the server is in a &quot;recording off&quot; state </span></div>
<div class="line"><span class="comment">// (ignore return values unequal RET_SUCCESS if </span></div>
<div class="line"><span class="comment">// recording is switched off already)</span></div>
<div class="line"><a class="code" href="group__functions.html#ga7c5046a1475133d1ab15d3d299878d65" title="Stops gaze data recording and scene video recording (if connected eye tracking device is HED)...">iV_StopRecording</a>(); </div>
<div class="line"></div>
<div class="line"><span class="comment">// clear the recording buffer to prevent old data from being saved</span></div>
<div class="line"><a class="code" href="group__functions.html#ga651c63470895aa6d098851a5725058fe" title="Clears the recorded data buffer.">iV_ClearRecordingBuffer</a>();</div>
<div class="line"></div>
<div class="line"><span class="comment">// Start data recording</span></div>
<div class="line"><a class="code" href="group__functions.html#ga9fb78edcbd972e37e31c99cee6cc867a" title="Starts gaze data recording and scene video recording (if connected eye tracking device is HED)...">iV_StartRecording</a>();</div>
<div class="line"></div>
<div class="line"><span class="comment">// ... record data ...</span></div>
<div class="line"></div>
<div class="line"><span class="comment">// At the end of an experiment:</span></div>
<div class="line"></div>
<div class="line"><span class="comment">// switch off recording </span></div>
<div class="line"><a class="code" href="group__functions.html#ga7c5046a1475133d1ab15d3d299878d65" title="Stops gaze data recording and scene video recording (if connected eye tracking device is HED)...">iV_StopRecording</a>();</div>
<div class="line"></div>
<div class="line"><span class="comment">// save data</span></div>
<div class="line"><a class="code" href="group__functions.html#ga9a80d3724f377a7ee903736282fcad1b" title="Writes recorded data buffer to disc.">iV_SaveData</a>(<span class="stringliteral">&quot;file_name.idf&quot;</span>, <span class="stringliteral">&quot;file desciption&quot;</span>, <span class="stringliteral">&quot;user&quot;</span>, 0);</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ul>
<li>Some devices (RED-m, RED, HiSpeed, HED, MRI/MEG) do not allow to call <a class="el" href="group__functions.html#ga7c5046a1475133d1ab15d3d299878d65">iV_StopRecording</a> if the eye tracking server is in a paused state.</li>
</ul>
</dd></dl>
<h3><a class="anchor" id="sendingmessages"></a>
Sending Messages</h3>
<p>After data has been recorded, it is often important to have information about certain events that happened during recording, e.g. when a new trial started or stimulus material changed. Data in IDF files can be marked with messages during a recording. The two functions that can be used to send messages are <a class="el" href="group__functions.html#ga29d317c3f998e6b19c1bd604593386f2">iV_SendImageMessage</a> and <a class="el" href="group__functions.html#gac9dbab07ddc1d7550a38daadf5e40c58">iV_ContinueRecording</a>. <a class="el" href="group__functions.html#ga29d317c3f998e6b19c1bd604593386f2">iV_SendImageMessage</a> can only be used while the recording is on, whereas <a class="el" href="group__functions.html#gac9dbab07ddc1d7550a38daadf5e40c58">iV_ContinueRecording</a> is used when the recording is currently paused.</p>
<p>Two types of messages can be send: <b> image messages </b> indicating an image or video file and <b> user messages </b> without such indication. To indicate an image or video file, an image message has to end on <code>.png</code>, <code>.jpg</code>, <code>.jpeg</code>, <code>.bmp</code> or <code>.avi</code>. When an IDF file is imported into BeGaze, image messages are used to set up the separation of data into different <b>trials</b>. Trials represent coherent recording periods of importance, e.g. when certain stimulus material was presented. User messages generate <b> user events </b> denoting certain events that happened during a trial, e.g. button clicks, the onset of an auditory stimulus or minor visual changes.</p>
<p>Consider the following example:</p>
<div class="fragment"><div class="line"><span class="comment">// Start the recording of data</span></div>
<div class="line"><a class="code" href="group__functions.html#ga9fb78edcbd972e37e31c99cee6cc867a" title="Starts gaze data recording and scene video recording (if connected eye tracking device is HED)...">iV_StartRecording</a>();</div>
<div class="line"></div>
<div class="line"><span class="comment">// send image message </span></div>
<div class="line"><a class="code" href="group__functions.html#ga29d317c3f998e6b19c1bd604593386f2" title="Sends a text message to iView X idf recording data file.">iV_SendImageMessage</a>(<span class="stringliteral">&quot;first_stimulus.jpg&quot;</span>);</div>
<div class="line"></div>
<div class="line"><span class="comment">// show first stimulus ...</span></div>
<div class="line"><span class="comment">// ... record data during the first stimulus ...</span></div>
<div class="line"></div>
<div class="line"><span class="comment">// send user message</span></div>
<div class="line"><a class="code" href="group__functions.html#ga29d317c3f998e6b19c1bd604593386f2" title="Sends a text message to iView X idf recording data file.">iV_SendImageMessage</a>(<span class="stringliteral">&quot;something happened!&quot;</span>);</div>
<div class="line"></div>
<div class="line"><span class="comment">// ... record more data during the first stimulus ...</span></div>
<div class="line"></div>
<div class="line"><span class="comment">// finish first stimulus</span></div>
<div class="line"><span class="comment">// pause the recording</span></div>
<div class="line"><a class="code" href="group__functions.html#gad5644021b75f2011e3d205b48346fe69" title="Pauses gaze data recording.">iV_PauseRecording</a>();</div>
<div class="line"> </div>
<div class="line"><span class="comment">// ... no data is being recorded</span></div>
<div class="line"></div>
<div class="line"><span class="comment">// send image message</span></div>
<div class="line"><a class="code" href="group__functions.html#gac9dbab07ddc1d7550a38daadf5e40c58" title="Continues gaze data recording.">iV_ContinueRecording</a>(<span class="stringliteral">&quot;second_stimulus.jpg&quot;</span>);</div>
<div class="line"></div>
<div class="line"><span class="comment">// show second stimulus ...</span></div>
<div class="line"><span class="comment">// ... record data during the second stimulus</span></div>
<div class="line"></div>
<div class="line"><span class="comment">// finish second stimulus</span></div>
<div class="line"><span class="comment">// switch off recording </span></div>
<div class="line"><a class="code" href="group__functions.html#ga7c5046a1475133d1ab15d3d299878d65" title="Stops gaze data recording and scene video recording (if connected eye tracking device is HED)...">iV_StopRecording</a>();</div>
</div><!-- fragment --><p>In this case, BeGaze creates two trials on import, the first for the <code>"first_stimulus.jpg"</code> and the second for <code>"second_stimulus.jpg"</code> image message. Also, a user event will appear inside the first trial representing the time when the user message <code>"something happened!"</code> was sent.</p>
<p>In general, when an image message is sent to indicate the beginning of a new trial, all data recorded after sending the message is associated with that trial until a new image message is sent or the recording is stopped and saved. If there is any image message sent during recording, all data recorded before the first image message is being discarded by BeGaze on import because it cannot be associated with a certain trial.</p>
<h3><a class="anchor" id="trialsandstimuli"></a>
Associated Stimulus Files</h3>
<p>Trials created by using image messages are meant to be associated with stimulus material in BeGaze. Usually, stimulus material consists of image files, though video files are also possible. The name of those image files should be the same as the content of the corresponding image messages. When associating image messages with images on import into BeGaze, similar trials from one or multiple participant can be grouped under one stimulus (image). Additionally, the stimulus image serves as a preview image or background image for different analysis options in BeGaze.</p>
<p>The following example illustrates the usage of stimulus pictures. Let's consider the sample code from the previous subsection creating two trials with the image messages <code>first_stimulus.jpg</code> and <code>second_stimulus.jpg</code>. Assuming there is data from two participants having seen both trials and also one participant who has seen both trials twice, there are three IDF files, <code>participant_1.idf</code>, <code>participant_2.idf</code> and <code>participant_3.idf</code>. When two image files, <code>first_stimulus.jpg</code> and <code>second_stimulus.jpg</code>, are provided during data import into BeGaze, all trials can be grouped under the corresponding stimulus (image). More specifically, both stimuli receive four trials, one from the first participant, one from the second and two from the third.</p>
<p>The following schema illustrates this process. </p>
<div class="image">
<img src="IDF_import.jpg" alt="IDF_import.jpg"/>
</div>
 <p>When finishing the import both stimuli and all three participants are shown in the BeGaze Dashboard. </p>
<div class="image">
<img src="BeGaze_import.png" alt="BeGaze_import.png"/>
</div>
 <dl class="section note"><dt>Note</dt><dd><ul>
<li>It is not necessary for image message's content to be identical with the file names of the image files intended for import. However, having identical designations facilitates BeGaze import using the automatic "New Experiment from Folder ..." feature.</li>
<li>Stimulus image files and IDF files should be stored in the same folder to facilitate import into BeGaze.</li>
<li>Image files need to have the same exact resolution the eye tracking server was using during data recording. This is usually the screen resolution of the monitor during the last validation. However, the server resolution can also be set manually by using <a class="el" href="group__functions.html#ga0db016addc12970063d8a14c2ebd5b76">iV_SetResolution</a>.</li>
</ul>
</dd></dl>
<p><br/>
 <hr/>
<h2><a class="anchor" id="licenceKey"></a>
Setting the License Key</h2>
<p>Some SMI OEM products require a license key to initialize communication between your application and the iView eye tracking server. Before your application opens a connection to the iView eye tracking server using <a class="el" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0">iV_Connect</a> or <a class="el" href="group__functions.html#ga94b848674c8a3167296a220e920b58d3">iV_ConnectLocal</a>, the license key has to be passed to the system using <a class="el" href="group__functions.html#ga13f93aa80b365fa7afbfc4c17d2f758a">iV_SetLicense</a>.</p>
<p><br/>
 <hr/>
<h2><a class="anchor" id="gazeInteraction"></a>
Gaze Interaction Design</h2>
<p>In this chapter we want to give you some hints for gaze interaction applications and help you to create a good user experience for your users. It always helps a lot to visualize or record gaze data from your users to see, how they look in the applications or in your use case.</p>
<p><b>Keep the Natural Movement of the Eyes in Mind</b></p>
<p>Your eyes continuously scan the environment and send this input to your brain, which creates the image that you can see. Keep this basic Eye Movement rule in mind, when you design the interaction for your application. Avoid too long fixation times to activate something. Don’t try to replace classic input devices like the Mouse or the Keyboard with gaze input. It feels very unnatural for your user to control your system only with your eyes. A good Gaze Interaction combines classic devices with gaze input in a smart way.</p>
<p><b>Prepare your UI Elements</b></p>
<p>To avoid wrong selections with your gaze it is important to adjust the User Interface to the needs of gaze based interactions. Create bigger AOI’s around your UI Elements and use strong signal color to highlight the selection. Create enough space between the selectable Items and avoid overlapping AOIs. Be careful with animations and effects. Use them to attract the attention of your user. Avoid a detailed gaze visualization while the user interacts with your application. It may confuse the user.</p>
<p><b>Avoid the Midas-Touch Problem</b></p>
<p>It is hard for a system to distinguish between a user's intention of triggering functionalities and his aim of simply exploring the user interface (UI) with his gaze. Due to this fact using eye tracking technology implies a major design compromise that is generally known as the Midas Touch Problem. Midas touch occurs when the visual exploration of the screen unintentionally activates gaze-based functionality. In order to achieve a pleasant interaction, make sure, that you trigger a gaze-based event only when the gaze remains in the area of interest (AOI) for a predefined dwell time or when a button is pressed on the controller while the user is focusing on the AOI.</p>
<p><br/>
 <hr/>
<h1><a class="anchor" id="secTutorials"></a>
Tutorials and Examples</h1>
<p>The SDK includes sample code and applications for any major environment. All example programs described in this SDK Guide are also provided as source code in the examples directory. If you want to develop your own eye tracking application, we recommend copying the example code into your development environment and use it as a starting point for your own development.</p>
<p><br/>
 <hr/>
<h2><a class="anchor" id="ccpp"></a>
Tutorial: Loading iViewXAPI.dll dynamically with C/C++</h2>
<p>There are two ways to integrate iViewXAPI.dll into your own C/C++ Application:</p>
<ul>
<li>use the Linker Library <b>iViewXAPI.lib</b> within Microsoft Visual Studio Projects.</li>
<li>use late or dynamic binding to access API functions.</li>
</ul>
<p>This tutorial describes the steps to realize the second option. You learn how to load the library iViewXAPI.dll dynamically and to how to access and run the function <a class="el" href="group__functions.html#ga94b848674c8a3167296a220e920b58d3">iV_ConnectLocal</a>.</p>
<p>First you have to declare function pointer types for the functions from iView X™ API:</p>
<div class="fragment"><div class="line"><span class="comment">// declare function prototype</span></div>
<div class="line"><span class="keyword">typedef</span> int (CALLBACK* iV_ConnectLocalType)();</div>
</div><!-- fragment --><p>This has to be repeated in an equivalent way for all the other functions that are used in your application. The next step is to load the iViewXAPI.dll.</p>
<div class="fragment"><div class="line"><span class="comment">//Load the dll and keep the handle</span></div>
<div class="line">HINSTANCE dllHandle = NULL;</div>
<div class="line">dllHandle = LoadLibrary(L<span class="stringliteral">&quot;iViewXAPI.dll&quot;</span>);</div>
</div><!-- fragment --><p>If this iViewXAPI.dll is loaded successfully, you have to search for the function with the corresponding name.</p>
<div class="fragment"><div class="line"><span class="comment">//Get pointer to our function using GetProcAddress:</span></div>
<div class="line">iV_ConnectLocalType iV_ConnectLocalPtr = NULL;</div>
<div class="line">iV_ConnectLocalPtr = (iV_ConnectLocalType)GetProcAddress( dllHandle, <span class="stringliteral">&quot;iV_ConnectLocal&quot;</span>);</div>
</div><!-- fragment --><p>Finally, you can run the function: </p>
<div class="fragment"><div class="line"><span class="comment">// execute the function</span></div>
<div class="line"><span class="keywordtype">int</span> retVal = iV_ConnectLocalPtr();</div>
<div class="line">cout &lt;&lt; <span class="stringliteral">&quot;iV_ConnectLocalPtr: &quot;</span> &lt;&lt; retVal &lt;&lt; endl;</div>
</div><!-- fragment --><p><br/>
 <hr/>
<h2><a class="anchor" id="csharpTut"></a>
Tutorial: Displaying the gaze cursor with C#</h2>
<p>The SDK includes the source code for the C# example program <b>HelloEyetracker</b> described in <a class="el" href="index.html#rundemo">Running the Demo</a>. The C# example was created using Microsoft Visual Studio 2013. This tutorial describes how to access the iView X™ API to display an overlay at the gaze position using C# and windows forms.</p>
<div class="image">
<img src="HelloEyetracker.png" alt="HelloEyetracker.png"/>
</div>
 <p>The first step is to integrate the iView X™ API into our own application. The following code shows how to declare external functions and data structs:</p>
<div class="fragment"><div class="line">[DllImport(<span class="stringliteral">&quot;iView XAPI.dll&quot;</span>)]</div>
<div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">extern</span> Int32 <a class="code" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0" title="Establishes a connection to the iView eye tracking server.">iV_Connect</a>(StringBuilder sendIP, <span class="keywordtype">int</span> sendPort, StringBuilder receiveIP, <span class="keywordtype">int</span> receivePort);</div>
<div class="line">[DllImport(<span class="stringliteral">&quot;iView XAPI.dll&quot;</span>)]</div>
<div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">extern</span> Int32 <a class="code" href="group__functions.html#ga63ea57fa0861aa4255d64822a2103087" title="Disconnects from iView eye tracking server.">iV_Disconnect</a>();</div>
<div class="line">[DllImport(<span class="stringliteral">&quot;iView XAPI.dll&quot;</span>)]</div>
<div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">extern</span> Int32 <a class="code" href="group__functions.html#ga2fd180b432ff6ab1d8064cfd4448247e" title="Updates data in rawDataSample with current eye tracking data.">iV_GetSample</a>(ref <a class="code" href="struct_sample_struct.html" title="This struct provides information about an eye data sample.">SampleStruct</a> sampleData);</div>
<div class="line"></div>
<div class="line"><span class="keyword">public</span> <span class="keyword">struct </span><a class="code" href="struct_eye_data_struct.html" title="This struct provides numerical information about eye data.">EyeDataStruct</a></div>
<div class="line">{</div>
<div class="line">    <span class="keyword">public</span> <span class="keywordtype">double</span> <a class="code" href="struct_eye_data_struct.html#a29197bfdd2bf54be3087cb8cad2bead8" title="horizontal gaze position on screen [pixel]">gazeX</a>, <a class="code" href="struct_eye_data_struct.html#a5ffbb74e0547dcf930b982323b21b708" title="vertical gaze position on screen [pixel]">gazeY</a>;     <span class="comment">// pupil gaze [pixel]</span></div>
<div class="line">    <span class="keyword">public</span> <span class="keywordtype">double</span> <a class="code" href="struct_eye_data_struct.html#a0450e5cd9b1b6252199dbd281faba1ae" title="pupil diameter [mm]">diam</a>;             <span class="comment">// pupil diameter [pixel/mm] (mm for RED devices)</span></div>
<div class="line">    <span class="keyword">public</span> <span class="keywordtype">double</span> <a class="code" href="struct_eye_data_struct.html#a1d7075b81c69505cac9f18cb4aa0b888" title="horizontal eye position relative to camera [mm]">eyePositionX</a>      <span class="comment">// horizontal eye position relative to camera (only for RED)</span></div>
<div class="line">    <span class="keyword">public</span> <span class="keywordtype">double</span> <a class="code" href="struct_eye_data_struct.html#a39814084e6dcc6bf07cb315e434088b6" title="vertical eye position relative to camera [mm]">eyePositionY</a>      <span class="comment">// vertical eye position relative to camera (only for RED)</span></div>
<div class="line">    <span class="keyword">public</span> <span class="keywordtype">double</span> <a class="code" href="struct_eye_data_struct.html#a99692993c0364efb40859035c8ad33bc" title="distance to camera [mm]">eyePositionZ</a>;     <span class="comment">// distance to camera (only for RED)</span></div>
<div class="line">};</div>
<div class="line"></div>
<div class="line"><span class="keyword">public</span> <span class="keyword">struct </span><a class="code" href="struct_sample_struct.html" title="This struct provides information about an eye data sample.">SampleStruct</a></div>
<div class="line">{</div>
<div class="line">    <span class="keyword">public</span> Int64 <a class="code" href="struct_sample_struct.html#a4a4dd79aa33066114eeebd2a450d52d2" title="timestamp of current gaze data sample [microseconds]">timestamp</a>;         <span class="comment">// timestamp of current gaze data sample [microseconds]</span></div>
<div class="line">    <span class="keyword">public</span> <a class="code" href="struct_eye_data_struct.html" title="This struct provides numerical information about eye data.">EyeDataStruct</a> <a class="code" href="struct_sample_struct.html#a47129a38f95739a52f6a4bc58c1a4fb5" title="stores information of the left eye (see EyeDataStruct for more information)">leftEye</a>;   <span class="comment">// eye data for left eye</span></div>
<div class="line">    <span class="keyword">public</span> <a class="code" href="struct_eye_data_struct.html" title="This struct provides numerical information about eye data.">EyeDataStruct</a> <a class="code" href="struct_sample_struct.html#a8351754118212dc347fe26a839b5dd22" title="stores information of the right eye (see EyeDataStruct for more information)">rightEye</a>;  <span class="comment">// eye data for left eye</span></div>
<div class="line">    <span class="keyword">public</span> Int32 <a class="code" href="struct_sample_struct.html#af7c9e67d1e75bc7ce4fdc0b769eb5fef" title="plane number of gaze data sample (only for HED HT)">planeNumber</a>;       <span class="comment">// plane number of gaze data sample (only HED HT)</span></div>
<div class="line">};</div>
</div><!-- fragment --><p>The class <b>EyeTrackingController</b> which is part of the example provides a C# interface for all functions and data structures provided by iView X™ API. We recommend to use this class in your own application.</p>
<p>An instance of this class is generated with</p>
<div class="fragment"><div class="line"><a class="code" href="group__enums.html#ga527c6fb056818564500428b45f2348f0" title="The enumeration ETDevice can be used in connection with iV_GetSystemInfo to get information about whi...">ETDevice</a> = <span class="keyword">new</span> SmiSample.EyeTrackingController();</div>
</div><!-- fragment --><p>The following code snippets show how to use several functions from the SDK. To establish a connection with the iView eye tracking server, you first have to start the server, if it has not already been done. After the server is running, a connection to the server can be established using the <b>Connect</b> button. This button triggers the following function:</p>
<div class="fragment"><div class="line"><span class="keyword">private</span> <span class="keywordtype">void</span> connect_Click(<span class="keywordtype">object</span> sender, EventArgs e)</div>
<div class="line">{</div>
<div class="line">    <span class="keywordtype">int</span> ret = 0;</div>
<div class="line">    <span class="keywordflow">try</span></div>
<div class="line">    {</div>
<div class="line">        <span class="comment">// connect to localhost server </span></div>
<div class="line">        ret = <a class="code" href="group__enums.html#ga527c6fb056818564500428b45f2348f0" title="The enumeration ETDevice can be used in connection with iV_GetSystemInfo to get information about whi...">ETDevice</a>.iV_Connect(<span class="keyword">new</span> StringBuilder(<span class="stringliteral">&quot;127.0.0.1&quot;</span>), Convert.ToInt32(<span class="stringliteral">&quot;4444&quot;</span>), <span class="keyword">new</span> StringBuilder(<span class="stringliteral">&quot;127.0.0.1&quot;</span>), Convert.ToInt32(<span class="stringliteral">&quot;5555&quot;</span>));</div>
<div class="line">        <span class="keywordflow">if</span> (ret == 1) logger.Text = <span class="stringliteral">&quot;iV_Connect: connection established&quot;</span>;</div>
<div class="line">        <span class="keywordflow">if</span> (ret != 1) logger.Text = <span class="stringliteral">&quot;iV_Connect: failed to establish connection: &quot;</span> + ret;</div>
<div class="line">    }</div>
<div class="line">    <span class="keywordflow">catch</span> (Exception exc)</div>
<div class="line">    {</div>
<div class="line">        logger.Text = <span class="stringliteral">&quot;Exception during iV_Connect: &quot;</span> + exc.Message;</div>
<div class="line">    }</div>
<div class="line">} </div>
</div><!-- fragment --><p>For RED-OEM devices setting a license key is required before a connection can be established. If you need to set a license key, enter the key in the desired text field and press the <b>Set License Key</b> button. This action will run the following function:</p>
<div class="fragment"><div class="line"><span class="keyword">private</span> <span class="keywordtype">void</span> key_Click(<span class="keywordtype">object</span> sender, EventArgs e)</div>
<div class="line">{</div>
<div class="line">    <span class="keywordtype">int</span> ret = 0;</div>
<div class="line">    <span class="keywordflow">try</span></div>
<div class="line">    {</div>
<div class="line">        <span class="comment">// setting license </span></div>
<div class="line">        <a class="code" href="group__enums.html#ga527c6fb056818564500428b45f2348f0" title="The enumeration ETDevice can be used in connection with iV_GetSystemInfo to get information about whi...">ETDevice</a>.iV_SetLicense(<span class="keyword">new</span> StringBuilder(licensekey.Text));</div>
<div class="line">        <span class="keywordflow">if</span> (ret == 1) logger.Text = <span class="stringliteral">&quot;iV_SetLicense: license set successfully&quot;</span>;</div>
<div class="line">        <span class="keywordflow">if</span> (ret != 1) logger.Text = <span class="stringliteral">&quot;iV_SetLicense: failed to set license: &quot;</span> + ret;</div>
<div class="line">    }</div>
<div class="line">    <span class="keywordflow">catch</span> (Exception exc)</div>
<div class="line">    {</div>
<div class="line">        logger.Text = <span class="stringliteral">&quot;Exception during iV_SetLicense: &quot;</span> + exc.Message;</div>
<div class="line">    }</div>
<div class="line">}</div>
</div><!-- fragment --><p>To achieve the best accuracy, each participant needs to calibrate individually. This calibration procedure can be individually changed using the combo boxes for the calibration method. To start the calibration process, click the <b>Calibrate</b> button. A calibration point will appear at the calibration area. The calibration point, which needs to be fixated by the participant, is moving from calibration point position to calibration point position until all points have been calibrated (by default 5 points).</p>
<div class="fragment"><div class="line"><span class="keyword">private</span> <span class="keywordtype">void</span> calibrate_Click(<span class="keywordtype">object</span> sender, EventArgs e)</div>
<div class="line">{</div>
<div class="line">    <span class="keywordtype">int</span> ret = 0;</div>
<div class="line">    <span class="keywordtype">int</span> calibrationPoints = 5;</div>
<div class="line">    <span class="keywordtype">int</span> targetSize = 20;</div>
<div class="line">    <span class="keywordflow">try</span></div>
<div class="line">    {</div>
<div class="line">        <span class="keywordflow">switch</span> (calibrationMethodComboBox.Text)</div>
<div class="line">        {</div>
<div class="line">        <span class="keywordflow">case</span> <span class="stringliteral">&quot;2P Calibration&quot;</span>:</div>
<div class="line">            calibrationPoints = 2;</div>
<div class="line">            <span class="keywordflow">break</span>;</div>
<div class="line">        <span class="keywordflow">default</span>:</div>
<div class="line">            <span class="keywordflow">case</span> <span class="stringliteral">&quot;5P Calibration&quot;</span>:</div>
<div class="line">            calibrationPoints = 5;</div>
<div class="line">            <span class="keywordflow">break</span>;</div>
<div class="line">        }</div>
<div class="line"></div>
<div class="line">        m_CalibrationData.displayDevice = 0;    <span class="comment">// only calibrate on the main monitor</span></div>
<div class="line">        m_CalibrationData.autoAccept = 1;</div>
<div class="line">        m_CalibrationData.method = calibrationPoints;</div>
<div class="line">        m_CalibrationData.visualization = 1;</div>
<div class="line">        m_CalibrationData.speed = 0;</div>
<div class="line">        m_CalibrationData.targetShape = 2;</div>
<div class="line">        m_CalibrationData.backgroundColor = 230;</div>
<div class="line">        m_CalibrationData.foregroundColor = 250;</div>
<div class="line">        m_CalibrationData.targetSize = targetSize;</div>
<div class="line">        m_CalibrationData.targetFilename = <span class="stringliteral">&quot;&quot;</span>;</div>
<div class="line"></div>
<div class="line">        ret = <a class="code" href="group__enums.html#ga527c6fb056818564500428b45f2348f0" title="The enumeration ETDevice can be used in connection with iV_GetSystemInfo to get information about whi...">ETDevice</a>.iV_SetupCalibration(ref m_CalibrationData);</div>
<div class="line">        <span class="keywordflow">if</span> (ret == 1) logger.Text = <span class="stringliteral">&quot;iV_SetupCalibration: calibration set up successfully&quot;</span>;</div>
<div class="line">        <span class="keywordflow">if</span> (ret != 1) logger.Text = <span class="stringliteral">&quot;iV_SetupCalibration: failed to setup calibration: &quot;</span> + ret;</div>
<div class="line">        <span class="keywordflow">if</span> (ret != 1) <span class="keywordflow">return</span>;</div>
<div class="line"></div>
<div class="line">        ret = <a class="code" href="group__enums.html#ga527c6fb056818564500428b45f2348f0" title="The enumeration ETDevice can be used in connection with iV_GetSystemInfo to get information about whi...">ETDevice</a>.iV_Calibrate();</div>
<div class="line">        <span class="keywordflow">if</span> (ret == 1) logger.Text = <span class="stringliteral">&quot;iV_Calibrate: calibration finished successfully&quot;</span>;</div>
<div class="line">        <span class="keywordflow">if</span> (ret != 1) logger.Text = <span class="stringliteral">&quot;iV_Calibrate: failed to calibrate: &quot;</span> + ret;</div>
<div class="line">    }</div>
<div class="line">    <span class="keywordflow">catch</span> (System.Exception exc)</div>
<div class="line">    {</div>
<div class="line">        logger.Text = <span class="stringliteral">&quot;Calibration Exception: &quot;</span> + exc.Message;</div>
<div class="line">    }</div>
<div class="line">}</div>
</div><!-- fragment --><p>To activate the live gaze cursor, you need to check the <b>Live Gaze</b> checkbox. This will activate an additional transparent fullscreen window form to show the live gaze cursor, which has the following properties:</p>
<div class="fragment"><div class="line">this.Opacity = 0.75f;</div>
<div class="line">this.BackColor = Color.LimeGreen;   <span class="comment">// trick to make a transparent background</span></div>
<div class="line">this.TransparencyKey = Color.LimeGreen;</div>
<div class="line">this.TopMost = <span class="keyword">true</span>;</div>
<div class="line">this.FormBorderStyle = FormBorderStyle.None;</div>
<div class="line">this.WindowState = FormWindowState.Maximized;</div>
<div class="line">this.DoubleBuffered = <span class="keyword">true</span>;</div>
<div class="line">this.ShowInTaskbar = <span class="keyword">false</span>;</div>
</div><!-- fragment --><p>The live gaze cursor will be painted on this window form within the <b>OnPaint</b> method, which will be called every time windows needs to repaint it.</p>
<div class="fragment"><div class="line"><span class="keyword">protected</span> <span class="keyword">override</span> <span class="keywordtype">void</span> OnPaint(PaintEventArgs e)</div>
<div class="line">{</div>
<div class="line">    base.OnPaint(e);</div>
<div class="line">    <span class="comment">// get the tracking data</span></div>
<div class="line">    <a class="code" href="group__enums.html#ga527c6fb056818564500428b45f2348f0" title="The enumeration ETDevice can be used in connection with iV_GetSystemInfo to get information about whi...">ETDevice</a>.iV_GetSample(ref rawDataSample);</div>
<div class="line">    <span class="comment">// average of left and right eye pos</span></div>
<div class="line">    <span class="keywordtype">float</span> posX = (float)(rawDataSample.leftEye.gazeX + rawDataSample.rightEye.gazeX) * 0.5f;</div>
<div class="line">    <span class="keywordtype">float</span> posY = (float)(rawDataSample.leftEye.gazeY + rawDataSample.rightEye.gazeY) * 0.5f;</div>
<div class="line">    <span class="comment">// move the pos to the middle of the circle</span></div>
<div class="line">    posX -= circleSize/2;</div>
<div class="line">    posY -= circleSize/2;</div>
<div class="line">    <span class="comment">// clamp the data to the screen resolution</span></div>
<div class="line">    <span class="keywordflow">if</span> (posX &gt; this.Width - circleSize)</div>
<div class="line">    {</div>
<div class="line">        posX = this.Width - circleSize;</div>
<div class="line">    }</div>
<div class="line">    <span class="keywordflow">else</span> <span class="keywordflow">if</span> (posX &lt; 0)</div>
<div class="line">    {</div>
<div class="line">        posX = 0;</div>
<div class="line">    }</div>
<div class="line">    <span class="keywordflow">if</span> (posY &gt; this.Height - circleSize)</div>
<div class="line">    {</div>
<div class="line">        posY = this.Height - circleSize;</div>
<div class="line">    }</div>
<div class="line">    <span class="keywordflow">else</span> <span class="keywordflow">if</span> (posY &lt; 0)</div>
<div class="line">    {</div>
<div class="line">        posY = 0;</div>
<div class="line">    }</div>
<div class="line">    <span class="comment">// draw the live gaze circle</span></div>
<div class="line">    e.Graphics.DrawEllipse(gazePen, posX, posY, circleSize, circleSize);</div>
<div class="line">}</div>
</div><!-- fragment --><p>You need to tell windows to update the form with the following function in your update loop:</p>
<div class="fragment"><div class="line">liveGazeForm.Invalidate();</div>
</div><!-- fragment --><p>When finishing your application, you need to disconnect the connection to server using the <a class="el" href="group__functions.html#ga63ea57fa0861aa4255d64822a2103087">iV_Disconnect</a> function.</p>
<div class="fragment"><div class="line"><span class="keyword">private</span> <span class="keywordtype">void</span> disconnecting()</div>
<div class="line">{</div>
<div class="line">    <span class="keywordtype">int</span> ret = 0;</div>
<div class="line">    <span class="keywordflow">try</span></div>
<div class="line">    {</div>
<div class="line">        ret = <a class="code" href="group__enums.html#ga527c6fb056818564500428b45f2348f0" title="The enumeration ETDevice can be used in connection with iV_GetSystemInfo to get information about whi...">ETDevice</a>.iV_Disconnect();</div>
<div class="line">        <span class="keywordflow">if</span> (ret == 1) logger.Text = <span class="stringliteral">&quot;iV_Disconnect: disconnected successfully&quot;</span>;</div>
<div class="line">        <span class="keywordflow">if</span> (ret != 1) logger.Text = <span class="stringliteral">&quot;iV_Disconnect: failed to disconnect: &quot;</span> + ret;</div>
<div class="line">    }</div>
<div class="line">    <span class="keywordflow">catch</span> (System.Exception exc)</div>
<div class="line">    {</div>
<div class="line">        logger.Text = <span class="stringliteral">&quot;Exception during iV_Disconnect: &quot;</span> + exc.Message;</div>
<div class="line">    }</div>
<div class="line">}</div>
</div><!-- fragment --> <hr/>
 <a class="anchor" id="matlab"></a><hr/>
<h2><a class="anchor" id="matlabSetup"></a>
Tutorial: MATLAB® Setup</h2>
<p>This tutorial describes the steps required to use the iView X™ API with Matlab 2014 or newer.</p>
<p>The examples scripts</p>
<ul>
<li><b>Gaze Contingent Experiment</b>: An example that demonstrates running a calibration session and subsequently recording eye tracking data. In this experiment gaze position data is retrieved from iView eye tracking server in real time and displayed as an overlay on the presented bitmap image. The example illustrates several example functions and commands and is a good starting point for writing your own eye tracking application.</li>
</ul>
<div class="image">
<img src="GazeContingentExample.png" alt="GazeContingentExample.png"/>
</div>
 <ul>
<li><b>Slide Show Experiment</b> runs a calibration session. Subsequently a series of images is presented to a user while eye tracking data is recorded in the background.</li>
</ul>
<ul>
<li><b>Gaze Contingent</b> demonstrate how to use the "psychophysics toolbox" in combination with eye tracking. Therefore it’s necessary to download and install version 3.0.11 of the "psychophysics toolbox" from <a href="http://psychtoolbox.org">http://psychtoolbox.org</a>. Read the "psychophysics toolbox" wiki for more information.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd>The toolbox is used for visualization purposes and is not required for communication with iView eye tracking server.</dd></dl>
<p>For using the iView X™ SDK without the "psychophysics toolbox" have a look into the examples</p>
<ul>
<li><b>DataStreaming</b>: Demonstrates how tor retrieve real time eye tracking data.</li>
<li><b>AOIhits</b>: Shows how gaze hits on AOIs can be analyzed</li>
</ul>
<p>Running the examples and using the provided source code may be a good starting point for developing own applications within MATLAB®. Within the examples there are four files that provide access to iViewXAPI.dll:</p>
<ul>
<li>InitiViewXAPI.m</li>
<li>iViewXAPI.m</li>
<li>UnloadiViewXAPI.m</li>
<li>InitAndConnectiViewXAPI.m</li>
</ul>
<p>Copy those files into your own project and you are able to access iView X™ API similar to the examples.</p>
<p><br/>
 <hr/>
<h2><a class="anchor" id="pythonSetup"></a>
Tutorial: Python Setup</h2>
<p>This tutorial describes the steps required to access iView X™ API from Python.</p>
<p>The iView X™ SDK includes four sample experiments for use with Python. To run the experiments <b>Slideshow</b> and <b>GazeContingent</b>, it is necessary to download and install <b>PsychoPy</b>. PsychoPy is an open source toolbox that allows presentation of stimuli and collection of data for a wide range of neuroscience, psychology and psychophysics experiments. In particular, PsychoPy provides Python specific visualizations being used in these examples. Please note that PsychoPy is NOT required for communication with iView eye tracking server, it is used for stimulus visualization in the said experiments. These Python examples were written with Python version 2.7.5. and PsychoPy version 1.73.06.</p>
<h3>Installing Prerequisites</h3>
<ol type="1">
<li>Python 2.7.5 or later versions from <a href="http://www.python.org">http://www.python.org</a> or any other source</li>
<li>Optional: PsychoPy from <a href="http://www.psychopy.org/">http://www.psychopy.org/</a> and additional libraries from<br/>
<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/">http://www.lfd.uci.edu/~gohlke/pythonlibs/</a><br/>
 or any other source<ol type="a">
<li>PsychoPy Toolbox 1.73.06</li>
<li>Numpy</li>
<li>Pyglet</li>
<li>Python Imaging library</li>
<li>wxpython</li>
<li>wxPython-common</li>
<li>Dateutil</li>
<li>Pyparsing</li>
</ol>
</li>
</ol>
<h3>Running Examples</h3>
<ol type="1">
<li>Start iView eye tracking server</li>
<li>Run Python script</li>
</ol>
<h3>Creating an Application</h3>
<p>The following code shows how to load the required SDK DLL, connecting to iView eye tracking server, retrieving data and disconnecting from iView eye tracking server:</p>
<div class="fragment"><div class="line">from ctypes <span class="keyword">import</span> *</div>
<div class="line"></div>
<div class="line"><span class="keyword">class </span>CEye(Structure):</div>
<div class="line">    _fields_ = [(&quot;gazeX&quot;, c_double),</div>
<div class="line">    (&quot;gazeY&quot;, c_double),</div>
<div class="line">    (&quot;diam&quot;, c_double),</div>
<div class="line">    (&quot;eyePositionX&quot;, c_double),</div>
<div class="line">    (&quot;eyePositionY&quot;, c_double),</div>
<div class="line">    (&quot;eyePositionZ&quot;, c_double)]</div>
<div class="line"></div>
<div class="line">class CSample(Structure):</div>
<div class="line">    _fields_ = [(&quot;timestamp&quot;, c_longlong),</div>
<div class="line">    (&quot;leftEye&quot;, CEye),</div>
<div class="line">    (&quot;rightEye&quot;, CEye),</div>
<div class="line">    (&quot;planeNumber&quot;, c_int)]</div>
<div class="line"></div>
<div class="line">leftEye = CEye(0,0,0)</div>
<div class="line">rightEye = CEye(0,0,0)</div>
<div class="line">sampleData = CSample(0,leftEye,rightEye,0)</div>
<div class="line">iViewXAPI = windll.LoadLibrary(&quot;iViewXAPI.dll&quot;)</div>
<div class="line">iViewXAPI.<a class="code" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0" title="Establishes a connection to the iView eye tracking server.">iV_Connect</a>(c_char_p(&#39;127.0.0.1&#39;), c_int(4444), c_char_p(&#39;127.0.0.1&#39;), c_int(5555))</div>
<div class="line">iViewXAPI.<a class="code" href="group__functions.html#ga2fd180b432ff6ab1d8064cfd4448247e" title="Updates data in rawDataSample with current eye tracking data.">iV_GetSample</a>(byref(sampleData))</div>
<div class="line">iViewXAPI.<a class="code" href="group__functions.html#ga63ea57fa0861aa4255d64822a2103087" title="Disconnects from iView eye tracking server.">iV_Disconnect</a>()</div>
</div><!-- fragment --><p>It's recommended to use the following files as wrappers to access the iView X™ SDK.</p>
<ul>
<li><b>iViewXAPI.py</b> demonstrates how to import the iView X™ SDK library and how to declare and initialize data structure that are needed for the use of the iView X™ SDK functions.</li>
<li><b>iViewXAPIReturnCodes.py</b> handles iView X™ SDK return codes.</li>
</ul>
<p><br/>
 <hr/>
<h2><a class="anchor" id="eprimeSetup"></a>
Tutorial: E-Prime Setup</h2>
<p>This tutorial describes the steps required to access iView X™ API from E-Prime.</p>
<p>The SDK includes several example experiments for E-Prime, two for the Standard version and two for the Professional version. The provided E-Prime sample experiments show you how to use this and other built-in E-Prime capabilities with the SDK functions.</p>
<p>The E-Prime examples were created with version 2.0.10.356 and can be converted to newer versions.</p>
<dl class="section note"><dt>Note</dt><dd>The iView X™ SDK provides a package file (.epk2) for E-Prime 2 Professional to simplify the writing of your own experiments. To make the package file available in E-Prime you have to set the package's path in the E-Prime options under "Tools" &rarr; "Options…" &rarr; "Packages". In "User Search Folders:" add the following path:</dd></dl>
<pre>
C:\[Program Files]\SMI\iView X SDK\bin
</pre><div class="image">
<img src="eprime.png" alt="eprime.png"/>
</div>
 <p>The following code shows how to declare structs and functions from the SDK that are needed for connecting to, getting a sample from and disconnecting from iView eye tracking server:</p>
<div class="fragment"><div class="line">Declare Function <a class="code" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0" title="Establishes a connection to the iView eye tracking server.">iV_Connect</a> Lib <span class="stringliteral">&quot;iviewxapi.dll&quot;</span> (ByVal sendIPAddress As String, ByVal sendPort As Long, ByVal recvIPAddress As String, ByVal readPort As Long) As Long</div>
<div class="line"></div>
<div class="line">Declare Function <a class="code" href="group__functions.html#ga63ea57fa0861aa4255d64822a2103087" title="Disconnects from iView eye tracking server.">iV_Disconnect</a> Lib <span class="stringliteral">&quot;iviewxapi.dll&quot;</span> () As Long</div>
<div class="line"></div>
<div class="line">Type <a class="code" href="struct_eye_data_struct.html" title="This struct provides numerical information about eye data.">EyeDataStruct</a></div>
<div class="line">    gazeX As Double</div>
<div class="line">    gazeY As Double</div>
<div class="line">    diam As Double</div>
<div class="line">    eyePosX As Double</div>
<div class="line">    eyePosY As Double</div>
<div class="line">    eyePosZ As Double</div>
<div class="line">End Type</div>
<div class="line"></div>
<div class="line">Type <a class="code" href="struct_sample_struct32.html" title="This struct provides information about a eye data samples.">SampleStruct32</a></div>
<div class="line">    timestamp As Double</div>
<div class="line">    leftEye As <a class="code" href="struct_eye_data_struct.html" title="This struct provides numerical information about eye data.">EyeDataStruct</a></div>
<div class="line">    rightEye As <a class="code" href="struct_eye_data_struct.html" title="This struct provides numerical information about eye data.">EyeDataStruct</a></div>
<div class="line">    planeNumber As Long</div>
<div class="line">End Type</div>
<div class="line"></div>
<div class="line">Declare Function <a class="code" href="group__functions.html#gadf55de8b84043a1fcd7afda7c4f5055e" title="Updates data in rawDataSample with current eye tracking data sample.">iV_GetSample32</a> Lib &quot;iviewxapi.dll&quot; (ByRef mySampleStruct As <a class="code" href="struct_sample_struct32.html" title="This struct provides information about a eye data samples.">SampleStruct32</a>) As Long</div>
</div><!-- fragment --><p>The following code shows how to connect to, get a gaze data sample and disconnect from iView eye tracking server:</p>
<div class="fragment"><div class="line">Dim ret As Long</div>
<div class="line"></div>
<div class="line">Dim sendIPAddress as String</div>
<div class="line">Dim recvIPAddress as String</div>
<div class="line">Dim sendPort As Long</div>
<div class="line">Dim readPort As Long</div>
<div class="line"></div>
<div class="line">sendPort = 4444</div>
<div class="line">readPort = 5555</div>
<div class="line">sendIPAddress = <span class="stringliteral">&quot;127.0.0.1&quot;</span></div>
<div class="line">recvIPAddress = <span class="stringliteral">&quot;127.0.0.1&quot;</span></div>
<div class="line"></div>
<div class="line">Dim sample As <a class="code" href="struct_sample_struct32.html" title="This struct provides information about a eye data samples.">SampleStruct32</a></div>
<div class="line"></div>
<div class="line"><span class="stringliteral">&#39; connect to iView X</span></div>
<div class="line"><span class="stringliteral">ret = iV_Connect (sendIPAddress, sendPort, recvIPAddress, readPort)</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">ret = iV_GetSample32 (sample)</span></div>
</div><!-- fragment --><p>It is recommended to use the custom calibration visualization feature and to render the calibration points with E-Prime functions. To get the current calibration point's position you will need to poll for the required data using <a class="el" href="group__functions.html#ga7b0c085be14605b1bcc520f36ecd53ef">iV_GetCurrentCalibrationPoint</a>. See <a class="el" href="index.html#pollingvscallbacks">Polling vs. Callbacks</a> for details.</p>
<p><br/>
 <hr/>
<h2><a class="anchor" id="nbsSetup"></a>
Tutorial: NBS Presentation Setup</h2>
<p>This tutorial describes the steps required to access iView X™ API from NBS Presentation.</p>
<p>Since the SMI NBS Presentation extension distributes two different Presentation interfaces, both will be treated as separate objects and needs to be instantiated individually in the script file:</p>
<table class="doxtable">
<tr>
<th>Interface </th><th>Class</th></tr>
<tr>
<td>EyeTracker2Impl </td><td>eye_tracker </td></tr>
<tr>
<td>PCLLibrary </td><td>iViewXAPI::eye_tracker2 </td></tr>
</table>
<p>NBS Presentation allows interacting with external hardware (such as eye tracking devices) using NBS Presentation extension. This extension (iViewXAPI_NBS.dll) is provided by SMI as a part of the iView X™ SDK and needs to be registered in the operation system before you can use it in the NBS Presentation experiments. There are two interfaces implemented in the delivered extension (EyeTracker2Impl and PCLLibrary) with individual functionality. While EyeTracker2Impl delivers the standard eye tracking functionality for NBS Presentation, like calibrating, validating, delivery of numerical data set, etc. the PCLLibrary extends this basic functionality by several functions which will be described below.</p>
<h3>Registering the extension</h3>
<p>Please follow the description below to register the NBS Presentation extension iViewXAPI_NBS.dll in Presentation:</p>
<ol type="1">
<li>In Presentation go to "Tools " &rarr; "Extension Manager".</li>
<li>In Extension Manager press "Select Extension File".</li>
<li>In the file browser that opens select the directory where iView X™ SDK is installed (typically <b>C:\Program Files\SMI\iView X SDK</b>). From this directory select subdirectory <b>bin</b>.</li>
<li>Select file <b>iViewXAPI_NBS.dll</b> and press <b>Open</b>. <div class="image">
<img src="nbs0.png" alt="nbs0.png"/>
</div>
 </li>
<li>In Extension Manager in <b>Available Extensions</b> select <b>EyeTracker2Impl</b>. <div class="image">
<img src="nbs1.png" alt="nbs1.png"/>
</div>
 </li>
<li>In <b>Register As:</b> type "1" (or any other unique name) <div class="image">
<img src="nbs2.png" alt="nbs2.png"/>
</div>
 </li>
<li>Press <b>Register Extension</b></li>
<li>Repeat steps 2 – 4.</li>
<li>In Extension Manager in <b>Available Extensions</b> select <b>PCLLibrary</b>. <div class="image">
<img src="nbs3.png" alt="nbs3.png"/>
</div>
 </li>
<li>In <b>Register As:</b> type "2" (or any other unique name, don’t use the same name as in step 6)</li>
<li>Press <b>Register Extension</b>. Afterwards the Extension Manager should show the situation as given in the picture below: <div class="image">
<img src="nbs4.png" alt="nbs4.png"/>
</div>
 </li>
<li>Close Extension Manager. For more information on Presentation extensions and the Extension Manager please visit the NBS website <a href="http://www.neurobs.com">http://www.neurobs.com</a>.</li>
</ol>
<h3>Available Functions</h3>
<p><b>EyeTracker2Impl Functionality</b></p>
<p><a class="el" href="group__nbs_eye_tracker2_impl.html">Functions implemented in EyeTracker2Impl for NBS Presentation</a> shows which functions will be supported by the SMI EyeTracker2Impl interface. See the Presentation Help 'eye tracker extension' for function description.</p>
<p><b>PCLLibrary Functionality</b></p>
<div class="fragment"><div class="line"><span class="preprocessor"># Establishes a connection to iView eye tracking server. </span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor"># connect will not return until a connection has been established. </span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor"># If no connection can be established, the function will return after the defined time span of 3 seconds. </span></div>
<div class="line"><span class="preprocessor"></span>errorhandle connect(<span class="keywordtype">string</span> sendIP, <span class="keywordtype">int</span> sendport, <span class="keywordtype">string</span> recvIP, <span class="keywordtype">int</span> recvport)</div>
<div class="line"></div>
<div class="line"><span class="preprocessor"># Disconnects from iView eye tracking server. </span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor"># disconnect will not return until the connection has been disconnected. </span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor"># After this function has been called no other function can communicate with iView eye tracking server. </span></div>
<div class="line"><span class="preprocessor"></span>errorhandle disconnect()</div>
<div class="line"></div>
<div class="line"><span class="preprocessor"># Writes recorded data buffer to disc. </span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor"># The filename can include the path. If the connected eye tracking device is a HED, scene video buffer is written too. </span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor"># save_data will not return until the data has been saved. </span></div>
<div class="line"><span class="preprocessor"></span>errorhandle save_data()</div>
<div class="line"></div>
<div class="line"><span class="preprocessor"># Returns horizontal accuracy with validated accuracy results. </span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor"># Before accuracy data is accessible the accuracy needs to be validated. </span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor"># If both eye data channels are available (binocular systems) the horizontal accuracy will be averaged. </span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor"># Invalid data will be marked as -1. </span></div>
<div class="line"><span class="preprocessor"></span><span class="keywordtype">double</span> get_accuracy_x()</div>
<div class="line"></div>
<div class="line"><span class="preprocessor"># Returns vertical accuracy with validated accuracy results. </span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor"># Before accuracy data is accessible the accuracy needs to be validated. </span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor"># If both eye data channels are available (binocular systems) the vertical accuracy will be averaged. </span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor"># Invalid data will be marked as -1. </span></div>
<div class="line"><span class="preprocessor">double get_accuracy_y()</span></div>
</div><!-- fragment --><h3>Data handling</h3>
<p>Due to consistency, the eye parameter handed over by functions start_data, stop_data should be equal to the parameter which will be handed over to functions like new_position_data, last_position_data, etc. and match the data which will be delivered by the SMI Eye Tracking device. If the parameters do not match the functions new_position_data might not provide any data.</p>
<h3>Using NBS Presentation</h3>
<p>The following code shows how to create instances of both extensions and how to use them.</p>
<div class="fragment"><div class="line"><span class="preprocessor"># create PCL extension instance and connect to the iView eye tracking server</span></div>
<div class="line"><span class="preprocessor"></span></div>
<div class="line">iViewXAPI::eye_tracker2 tracker2 = <span class="keyword">new</span> iViewXAPI::eye_tracker2( <span class="stringliteral">&quot;{B7A4A7F7-7879-4C95-A3BA-6CCB355AECF6}&quot;</span> );</div>
<div class="line">tracker2.connect(iViewX_IP, Send_Port, Local_IP, Recv_Port);</div>
<div class="line"></div>
<div class="line"><span class="preprocessor"># create eye tracker extension instance, start tracking and start deliver gaze position data </span></div>
<div class="line"><span class="preprocessor"></span></div>
<div class="line">eye_tracker tracker = <span class="keyword">new</span> eye_tracker( <span class="stringliteral">&quot;{FDC35980-7480-4761-859F-4DCCFA93BA57}&quot;</span> );</div>
<div class="line">tracker.start_tracking();</div>
<div class="line">tracker.start_data(dt_position);</div>
<div class="line"></div>
<div class="line"><span class="preprocessor"># start calibration using a predefined calibration method, acceptation and speed setting, and start IDF data recording</span></div>
<div class="line"><span class="preprocessor"></span></div>
<div class="line">tracker.calibrate( et_calibrate_default, calibration_method, calibration_auto_accept, calibration_speed);</div>
<div class="line">tracker.set_recording (<span class="keyword">true</span>);</div>
<div class="line"></div>
<div class="line"><span class="preprocessor"># get the current gaze position data </span></div>
<div class="line"><span class="preprocessor"></span></div>
<div class="line"><span class="keywordflow">if</span>( tracker.new_position_data() != 0 ) then</div>
<div class="line">     eyepos = tracker.last_position_data();</div>
<div class="line">end;</div>
<div class="line"></div>
<div class="line"><span class="preprocessor"># stop IDF data recording and save the recorded data to a predefined file </span></div>
<div class="line"><span class="preprocessor"></span></div>
<div class="line">tracker.set_recording(<span class="keyword">false</span>);</div>
<div class="line">tracker2.save_data(<span class="stringliteral">&quot;presentation_data.idf&quot;</span>, <span class="stringliteral">&quot;description&quot;</span>, <span class="stringliteral">&quot;user&quot;</span>, 1);</div>
<div class="line"></div>
<div class="line"><span class="preprocessor"># disconnect from iView eye tracking server</span></div>
<div class="line"><span class="preprocessor"></span></div>
<div class="line">tracker2.disconnect()</div>
</div><!-- fragment --><p>Before getting started with the NBS Presentation example experiments included with the SDK, please verify that the following settings match your current setup:</p>
<p>(1) Display Device</p>
<p>The Display Device settings, which may be found under the <b>Settings</b> tab and Video Option, should match the actual display output setting of your environment. For example, if you will be displaying your NBS Presentation experiment on your primary monitor, the Primary Display Driver and according display mode must be selected. In the example below the display mode is 1680x1050x32 (60 Hz). If you are displaying your experiment on a secondary monitor, select the Secondary Display Driver option from the <b>Adapter</b> drop-down menu.</p>
<div class="image">
<img src="nbs5.png" alt="nbs5.png"/>
</div>
 <p>(2) Screen Resolution Settings</p>
<p>The Screen Resolution Settings for the NBS Presentation experiments are set in the .sce file. Please make sure that the values set forth in the Display Device settings illustrated above match those in the .sce file. In the example below, the screen resolution is set to 1680x1050.</p>
<p>(3) Network Connection Settings</p>
<p>The Network Connection Settings for the NBS Presentation experiments are set in the .pcl file. Please verify that settings here match those set forth in iView X™ (Setup &rarr; Hardware &rarr; Communication &rarr; Ethernet), iView RED-m (Tray Icon &rarr; Network Connection) or iViewRED (Settings Tab). Otherwise, the NBS Presentation experiment will not be able to communicate with the Eye Tracking Server. As mentioned previously, if you are configuring your eye tracker to run in a dual PC setup, the connection settings must reflect such (i.e., the actual IP addresses and ports must be listed).</p>
<div class="fragment"><div class="line"><span class="preprocessor">#############################################################</span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor">#                                                           #</span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor">#   choose connection settings to                           #</span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor">#   establish communication with iView eye tracking server  #</span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor">#                                                           #</span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor">#############################################################</span></div>
<div class="line"><span class="preprocessor"></span></div>
<div class="line"><span class="preprocessor"># connection settings</span></div>
<div class="line"><span class="preprocessor"></span><span class="keywordtype">string</span> iViewX_IP = <span class="stringliteral">&quot;127.0.0.1&quot;</span>;</div>
<div class="line"><span class="keywordtype">string</span> Local_IP = <span class="stringliteral">&quot;127.0.0.1&quot;</span>;</div>
<div class="line"><span class="keywordtype">int</span> Send_Port = 4444;</div>
<div class="line"><span class="keywordtype">int</span> Recv_Port = 5555;</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>The Presentation Interface included with the SMI iTools package does NOT need to be nor should it be used in combination with the SDK to enable communication between iView eye tracking server and NBS Presentation. In fact, they are separate packages. Communication may be enabled with NBS Presentation directly through use of the SDK. While the Presentation Interface contains useful commands for start/stop recording and handling of the calibration process, we recommend that you use the SDK due to its more expansive feature set and capabilities.</dd></dl>
<p><br/>
 <hr/>
<h2><a class="anchor" id="unity"></a>
Tutorial: Getting started with Unity</h2>
<p>This tutorial describes the steps required to use the iView X™ API in Unity. Unity is a cross-platform game creation tool by Unity Technologies, including a 3D and 2D game engine and an integrated development environment. With Unity you can create various applications like games, simulations or training-software. The Unity extension of the iView X™ SDK works with all Unity editions and plans (Personal, Plus, Pro, Enterprise), but the use of the created Unity applications is restricted to the supported platforms of the used SMI eye tracker. Please refer to the respective user manual for details.</p>
<p>The Unity extension of the iView X™ SDK is designed for Unity 5.0 and later versions.</p>
<p>You can download the latest version of Unity from: <a href="http://unity3d.com/unity/download">http://unity3d.com/unity/download</a></p>
<p>The system requirements for Unity can be found here: <a href="http://unity3d.com/unity/system-requirements">http://unity3d.com/unity/system-requirements</a></p>
<h3><a class="anchor" id="unitySetup"></a>
Setup</h3>
<p>The Unity integration can be added to an existing Unity project by importing all the assets contained in the package <b>Plugin.unitypackage</b>. This package is located in the iView X™ SDK folder <br/>
<b>\iView X SDK\Examples\Unity</b></p>
<p>The SDK also contains an example Unity project demonstrating the usage of the integration for different use cases. To explore the examples, copy the project folder located in the iView X™ SDK folder <b>\iView X SDK\Examples\Unity\Examples</b> to a new location of your choice and open it with the Unity Editor.</p>
<p>The example project contains a <b>menu</b> scene that allows you to start each example. Navigate to the <b>menu</b> scene and run the project. You can also select each example scene individually from the Unity project view and run it directly. Use the [ESC] key from within each example to return to the <b>menu</b> scene.</p>
<div class="image">
<img src="Menu_Unity.png" alt="Menu_Unity.png"/>
<div class="caption">
Unity Examples menu</div></div>
<p>The following table provides an overview of the content of the examples:</p>
<table class="doxtable">
<tr>
<th>Name </th><th>Content</th></tr>
<tr>
<td>01_Gaze&amp;NewGUISystem </td><td>Shows how to use the gaze with the new GUI-System (since Unity 4.6). The icons will grow when looked at. </td></tr>
<tr>
<td>02_Gaze&amp;Sprites </td><td>Similar to the first example, but internally using the Unity sprite engine for the interaction. </td></tr>
<tr>
<td>03_Gaze&amp;LegacyGUI </td><td>Uses the legacy GUI-System of Unity (before Unity 4.6). The icon will change, when looked at. </td></tr>
<tr>
<td>04_Gaze&amp;3DWorld </td><td>Shows how to use the gaze in the Unity 3D world. The cubes will grow when looked at. </td></tr>
<tr>
<td>05_GazeMonoExample </td><td>Example for the class <a class="el" href="index.html#unityGazeMonoBehavior">GazeMonoBehaviour</a>. Look at one of the boxes and use the [Space] key to shoot the boxes. </td></tr>
</table>
<p>All examples provide the following keyboard shortcuts:</p>
<ul>
<li>Press [1] to start a 1-point calibration.</li>
<li>Press [2] to start a 5-point calibration.</li>
<li>Press [3] to start a validation.</li>
</ul>
<p>You can change the default key assignment in the Inspector and in the <code>SMIGazeController</code> class:</p>
<div class="image">
<img src="SMIGazeController.PNG" alt="SMIGazeController.PNG"/>
<div class="caption">
SMIGazeController properties</div></div>
 <h3><a class="anchor" id="unityGetData"></a>
Getting data and using functions from the SDK</h3>
<p>The following code example shows, how you access the data from the eye tracker in your Unity script code:</p>
<div class="fragment"><div class="line"><span class="comment">// get the latest gaze data from the eye tracking server</span></div>
<div class="line">SampleData sample = SMIGazeController.Instance.GetSample();</div>
<div class="line">            </div>
<div class="line"><span class="comment">// get the averaged GazePosition</span></div>
<div class="line">Vector3 averageGazePosition = sample.averagedEye.gazePositionInUnityScreenCoordinatespace(); </div>
</div><!-- fragment --><p>The SDK provides the gaze information in three different cooridinate spaces:</p>
<table class="doxtable">
<tr>
<th>Name </th><th>Description</th></tr>
<tr>
<td>gazePosInUnityScreenCoords(); </td><td>Returns the gaze position in the Unity screen coordinate system. The origin (0, 0) is the bottom left. (in px) </td></tr>
<tr>
<td>gazePosInScreenCoords(); </td><td>Returns the gaze position in the Windows screen coordinate system. The origin (0, 0) is top left. (in px) </td></tr>
<tr>
<td>gazePosInViewPortCoords() </td><td>Returns the gaze position in the Windows screen coordinate system, scaled to the view port size. The origin (0, 0) is top left. (in px) </td></tr>
</table>
<p>Now you can use the obtained gaze vector to raycast into the scene and get e.g. the name of the selected object:</p>
<div class="fragment"><div class="line">Ray rayGaze = Camera.main.ScreenPointToRay(averageGazePosition);</div>
<div class="line">RaycastHit hit; </div>
<div class="line"></div>
<div class="line"><span class="comment">// Raycast from the gaze position on the screen</span></div>
<div class="line"><span class="keywordflow">if</span>(Physics.Raycast(rayGaze, out hit))</div>
<div class="line">{</div>
<div class="line">    <span class="comment">// Print the name of the object in the raygaze</span></div>
<div class="line">    Debug.Log(<span class="stringliteral">&quot;The name of the object is: &quot;</span> + hit.collider.gameObject.name);</div>
<div class="line">}</div>
</div><!-- fragment --><p>The Unity Integration provides a simple way to get the current selection of the gaze. The following example shows, how to obtain the focused object:</p>
<div class="fragment"><div class="line">GameObject objectInFocus = SMIGazeController.Instance.GetObjectInFocus(FocusFilter.WorldSpaceObjects);</div>
</div><!-- fragment --><p>To start a calibration or a validation procedure using the following code:</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> calibrationType = 5;</div>
<div class="line"></div>
<div class="line"><span class="comment">// Start a calibration</span></div>
<div class="line">SMIGazeController.Instance.StartCalibration(calibrationType);</div>
<div class="line"></div>
<div class="line"><span class="comment">// Start a validation</span></div>
<div class="line">SMIGazeController.Instance.StartValidation();</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>Please have a look into the iView eye tracking software's manual to find out which calibration types are supported for your device.</dd></dl>
<h3><a class="anchor" id="unityGazeMonoBehavior"></a>
GazeMonoBehaviour</h3>
<p>The integration package contains an implementation of a <code>MonoBehaviour</code> with basic functions for gaze interaction applications. To use it, derive your own class from the class <code>GazeMonoBehaviour</code> and overwrite the basic functions <code>OnGazeEnter()</code>, <code>OnGazeStay()</code> and <code>OnGazeExit()</code>. Use these functions to define how the application reacts, if the gaze hits or leaves a game object. You can use the parameter <code>RaycastHit</code> of the function to get more information about the Raycast. Please have a look into the Unity Documentation for more information about the <code>RaycastHit</code> class (<a href="http://docs.unity3d.com/ScriptReference/RaycastHit.html">http://docs.unity3d.com/ScriptReference/RaycastHit.html</a>)</p>
<p>The following example illustrates the basic usage:</p>
<div class="fragment"><div class="line"><span class="keyword">using</span> iView;</div>
<div class="line"><span class="keyword">using</span> UnityEngine;</div>
<div class="line"></div>
<div class="line"></div>
<div class="line"><span class="comment">// Derive from GazeMonobehaviour</span></div>
<div class="line"><span class="keyword">public</span> <span class="keyword">class </span>GazeExample_GazeMonoExample : GazeMonobehaviour</div>
<div class="line">{</div>
<div class="line">    <span class="keyword">public</span> <span class="keyword">override</span> <span class="keywordtype">void</span> OnGazeEnter(RaycastHit hitInformation)</div>
<div class="line">    {</div>
<div class="line">        <span class="comment">// is called, when the gaze initially hits the object</span></div>
<div class="line">        base.OnGazeEnter(hitInformation);</div>
<div class="line">    }</div>
<div class="line"></div>
<div class="line">    <span class="keyword">public</span> <span class="keyword">override</span> <span class="keywordtype">void</span> OnGazeStay(RaycastHit hitInformation)</div>
<div class="line">    {</div>
<div class="line">        <span class="comment">// is called, when the gaze stays on the object</span></div>
<div class="line">        base.OnGazeStay(hitInformation);</div>
<div class="line">    }</div>
<div class="line"></div>
<div class="line">    <span class="keyword">public</span> <span class="keyword">override</span> <span class="keywordtype">void</span> OnGazeExit()</div>
<div class="line">    {</div>
<div class="line">        <span class="comment">// is called, when the gaze leaves the object</span></div>
<div class="line">        base.OnGazeExit();</div>
<div class="line">    }</div>
<div class="line">}</div>
</div><!-- fragment --><p><br/>
 <hr/>
<h1><a class="anchor" id="faq"></a>
Frequently Asked Questions</h1>
<ul>
<li><a class="el" href="index.html#faqTypicalErrors">What can I do if iV_Connect fails?</a></li>
</ul>
<h2><a class="anchor" id="faqTypicalErrors"></a>
What can I do if iV_Connect fails?</h2>
<ul>
<li>Check, if the eye tracking_server is running. Use the SMI iViewRED client to check if your system works correctly.</li>
<li>If you are using an SMI OEM eye tracking device, please make sure you have set the matching license code using <a class="el" href="group__functions.html#ga13f93aa80b365fa7afbfc4c17d2f758a">iV_SetLicense</a> before calling <a class="el" href="group__functions.html#ga63e7c030469ea2fef026ccd8a37cfae0">iV_Connect</a> or <a class="el" href="group__functions.html#ga94b848674c8a3167296a220e920b58d3">iV_ConnectLocal</a> </li>
</ul>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Fri Jan 27 2017 14:34:04 for iView X SDK by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.2 </li>
  </ul>
</div>
</body>
</html>
